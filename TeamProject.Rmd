---
title: "Team Project"
output: pdf_document
author: 
  - José Ignacio Díez Ruiz	100487766
  - Carlos Roldán Piñero	100484904
  - Pablo Vidal Fernández	100483812
date: "`r Sys.Date()`"
header-includes:
  - \renewcommand{\and}{\\}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
require(ggplot2)
require(tidyverse)
require(mice)
require(visdat)
require(MASS)
require(pracma)
require(rrcov)
require(corrplot)
require(GGally)
require(class)
require(moments)
require(nnet)
require(naivebayes)
require(pROC)
require(caret)
require(kableExtra)
```

<!-- TODO: Introduction -->

# Descriptive Analysis and Preprocessing

## Reading the data and setting the NAs

Before starting the analysis, we need to make some preprocessing on our data.
Let us start by loading it into memory and listing the names of the columns.

```{r}
data <- read.csv("diabetes.csv")
colnames(data)
```

Of these, our target variable is `Outcome`, which has two levels.
For convenience, we transform it into a factor variable which R can trat accordingly.

```{r}
data$Outcome <- factor(data$Outcome, c(0, 1), c("Negative", "Positive"))
```

<!-- Up to this point this would go in the introduction -->

Now we need to address a particularity of the chosen data:
not-a-number (NaN) instances are encoded as zeros in variables
where that value is imposible [^1].
These are:

- Glucose

- BloodPressure

- SkinThickness

- Insulin

- BMI

In order for us to later trat them correctly, we need to manually
change them to the existing `NA` type.
As we do so, we record the number of NaNs instances in each of those variables.
For convenience, we define a function.

[^1]: Remember we are dealing with medical data, not with artificial one.
Hence, there are constraints on the values a variable may take.

```{r}
set_nas <- function(data, fields) {
    percentage <- list()
    for (field in fields) {
        data[[field]][data[[field]] == 0]   <- NA
        percentage[[field]] <- 100 * sum(is.na(data[[field]])) / nrow(data)
    }
    return(list(data = data, percentage = percentage))
}

# Correctly label NaNs
na_fields   <- c("Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI")
data_na     <- set_nas(data, na_fields)
data        <- data_na$data
percentages <- data_na$percentage

# Visualize them
vis_miss(data)
```

Now the next logical step is to impute those NaN values.
We do have some concern about the imputation of the "Insulin" variable
which is almost half-filled of NaNs.
Nevertheless, we decide to impute it.
On the followed strategy, we use the "Predictive Mean Matching Imputation"
(PMM in short) as it behaves much more robustly than naive
mean or median imputations.

```{r}
data_im <- mice(data, m = 1, method = "pmm")
data    <- complete(data_im)
```

## Visualization of the data

Before we proceed any further, we are going to describe our data.
First, we look individually to each of the attributes,
we see both visually and with with a two-sample Wilcox-test whether there
is significant difference between the two groups (having or not diabetes),
and if some transformation may be desirable to ensure normality compatibility.
For that purpose, we define the following function.

```{r}
histogram_by_groups <- function(data, var, label = NULL) {
    stat_t <- wilcox.test(as.formula(paste(var, "~ Outcome")), data)
    data0  <- data[data$Outcome == "Negative", ]
    data1  <- data[data$Outcome == "Positive", ]
    if (is.null(label)) {
        label <- var
    }
    p <- ggplot(data0, aes(x = eval(parse(text = var)))) +
        geom_histogram(
            aes(y = after_stat(count / sum(count)), fill = "Negative"),
            bins = 10, colour = "white", alpha = 0.8, boundary = 0
            ) +
        geom_histogram(data = data1,
            aes(
                x = eval(parse(text = var)),
                y = after_stat(count / sum(count)), fill = "Positive"
                ),
            bins = 10, colour = "white",
            alpha = 0.5, boundary = 0, inherit.aes = FALSE) +
            theme_bw() +
            scale_fill_manual(
                name = "",
                breaks = c("Positive", "Negative"),
                values = c("Positive" = "deeppink4", "Negative" = "pink2")
            ) +
            xlab(label) + ylab("Relative frequency") + ggtitle(label) +
            geom_vline(xintercept = mean(data1[[var]]), colour = "deeppink4") +
            geom_vline(xintercept = mean(data0[[var]]), colour = "pink2")
    p + annotate(
            "text",
            x = 0.9 * max(data1[var]),
            y = 0.9 * max(ggplot_build(p)$data[[1]]["y"]),
            label = sprintf("p-value = %.4e", stat_t$p.value),
            size = 3
        )
}
```

Let us start with the number of pregnancies.
We can see that people who have diabetes have had more
pregnancies than those who do not have diabetes.
We see that it seems to be somewhat based on the p-value alone.
We also note it exhibits a heavily right-skewed behaviour.
As such, a logarithmic transformation would make sense to get
a distribution more compatible with the normal one.
Nonetheless, a problem here arises in dealing with the null values [^2].
For that purpose we shift the variable by one unit.
As a consistency measure, we will apply this shift to all
variables we log-transform.

[^2]: Remember the domain of the logarithmic function is $(0, \infty)$.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Pregnancies")
```

The change as we see does help in obtaining a more
centered distribution with which we may better apply
the posterior analysis.

```{r, fig.dim = c(8, 4)}
data$LogPregnancies <- log(data$Pregnancies + 1)
histogram_by_groups(data, "LogPregnancies")
```

The next variable to visualize is the glucose.
People with diabetes exhibit higher glucose values.
The p-value  is very small which
indicates a highly significant difference between the two groups.
We also observe that the distribution is already
well-centered and resembles a normal distribution.
Hence, we decide not to transform the data.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Glucose")
```

We move onwards to blood pressure.
In this case, although the distributions appear
as normal, there does not seem to exist a highly significant
difference between the groups in contrast to what the p-value states,
more so compared with the glucose variable.
Nonetheless, there seem to be an slight indication
of higher blood pressure for people with diabetes.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "BloodPressure")
```

Now skin thickness is interesting, because the distributions
are visually to those of the blood pressure but the presence
of outliers is appreciable.
We will later deal with those but for now let us
mantain this variable as it is.
Note that the population with diabetes appear
to exhibit a thicker skin.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "SkinThickness")
```

Insulin, as we may have expected from the name of the property
itself, appears to be a relevant.
The median of the distributions does indeed seem to differ,
with the one for the diabetes population being
slightly higher.
It is also right-skewed, so we decide to log-transform it.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Insulin")
```

It does appear that the transformation impproves the
the symmetrization of the data, although some left-skewness
appears.
We will later see if outlier detection get to
target those values or not.

```{r, fig.dim = c(8, 4)}
data$LogInsulin <- log(data$Insulin + 1)
histogram_by_groups(data, "LogInsulin")
```

Body Mass Index (BMI) again exhibits this tendency of
leaning towards a more right-skewed distribution.
It does also follow the tendency of being
slightly higher for people with diabetes.
As such we log-transform to try and get a more normalized variable.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "BMI")
```

As we may visually judge, it is the case
that the log transformation centers the data
and provides a more normal-like distribution.
There is some presence of seemingly outlying points
to the right.

```{r, fig.dim = c(8, 4)}
data$LogBMI <- log(data$BMI + 1)
histogram_by_groups(data, "LogBMI")
```

The Diabetes Pedigree Function (DPF) is the
again a flagrant right-skewed.
It again has higher values for the positive set.
This is to be expected from the definition of this
very function as a risk indication for diabetes.
Let us try to log-transform it.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "DiabetesPedigreeFunction", "DPF")
```

The improvement is highly noticeable.
We retain thus this transformed variable.

```{r, fig.dim = c(8, 4)}
data$LogDPF <- log(data$DiabetesPedigreeFunction + 1)
histogram_by_groups(data, "LogDPF")
```

We may note that young people, as with other illnesses
have less tendency to suffer diabetes than the elders.
The age is expected to exhibit a right-skewed
distribution, as is indeed the case.
In an attempt to improve the symmetry,
we once again use logarithms to transform the variable.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Age")
```

The transformation does help although
not by much.
This is a common problem of the age variable.
We keep the transformation anyway as it does seem
to help with the symmetry for the positive group.

```{r, fig.dim = c(8, 4)}
data$LogAge <- log(data$Age + 1)
histogram_by_groups(data, "LogAge")
```

```{r}
# Some convenience
df  <- subset(data, select = -c(Pregnancies, Insulin, BMI, DiabetesPedigreeFunction, Age))
df0     <-  df[df$Outcome == "Negative", ]
df1     <-  df[df$Outcome == "Positive", ]
xnames  <- names(df)[! names(df) %in% c("Outcome")]
```

Now, let's take a look at some multivariate plots. We'll begin by inspecting the Parallel Coordinate Plot:

```{r, fig.dim = c(8, 4)}
par(las = 2)
parcoord(df[xnames], col = c("pink2", "deeppink4"))
legend("topright", legend = c("Negative", "Positive"),
       col = c("pink2", "deeppink4"), lty = 1, lwd = 2)
```

It seems that, overall, the positive lines are 
over negative ones. 
This is most notable on the Glucose and log
transformed BMI.
They are two of the most significant
features according to the p-value.

The Andrew's plot is the following: 

```{r, fig.dim = c(8, 4)}
andrewsplot(as.matrix(df[xnames]), df$Outcome, style = "cart")
legend("topright", legend = c("Negative", "Positive"),
       col = c("black", "red"), lty = 1, lwd = 2)
```

Again, we see that the two groups are different.
The group of people who have diabetes tend to have more volatile curves,
reaching higher and lower values along the curve.

## Multivariate characteristics and outlier identification

We are goig to use a multivariate approach
to identifying the outliers in our data.
In the process, we will need to compute the
mean and covariance.

We start then by finding the mean vector and the
covariance matrix.
In order to reduce the sensitivity to outliers in this
computation, we use a robust estimation using 
the "Fast MCD" (Minimum Covariance Determinant) estimator.
We set main parameter, `alpha`, which determines the percentage
of the data to use, at $0.85$.

<!-- TODO: refactor the prints -->

```{r}
our_corrplot <- function(cov_mat) {
    colnames(cov_mat) <- c("Log\nPregnancies",
                           "Glucose",
                           "Blood\nPressure",
                           "Skin\nThickness",
                           "LogInsulin",
                           "LogBMI",
                           "LogDPF",
                           "LogAge")
    corrplot.mixed(cov2cor(cov_mat), lower = "number", upper = "color",
        diag = "n", tl.col = "black", tl.cex = 0.65,
        lower.col = "black")
}

mcd_est <- CovMcd(df[xnames], alpha = 0.85, nsamp = "deterministic")
```

```{r, echo = FALSE, fig.size = c(8,8)}
print("The mean vector is:")
print(mcd_est$center)
print("The covariance matrix is:")
print(round(mcd_est$cov, 4))
```

```{r, fig.size = c(8,8)}
our_corrplot(mcd_est$cov)
```

It is interesting to seggregate by the class
of the Outcome variable.
This way, we can both, get the mean vector and covariance
matrix for each of the classes, and the outliers for both sets.

```{r, fig.size = c(8,8)}
mcd_neg <- CovMcd(df0[xnames], alpha = 0.85, nsamp = "deterministic")
mcd_pos <- CovMcd(df1[xnames], alpha = 0.85, nsamp = "deterministic")
```

```{r, echo = FALSE, fig.size = c(8,8)}
print("### Negative class ###")
print("The mean vector is:")
print(mcd_neg$center)
print("The covariance matrix is:")
print(round(mcd_neg$cov, 4))
```

```{r, fig.size = c(8,8)}
our_corrplot(mcd_neg$cov)
```

```{r, echo = FALSE, fig.size = c(8,8)}
print("### Positive class ###")
print("The mean vector is:")
print(mcd_pos$center)
print("The covariance matrix is:")
print(round(mcd_pos$cov, 4))
```

```{r, fig.size = c(8,8)}
our_corrplot(mcd_pos$cov)
```

Let us focus first on the mean vectors.
The here observed particularities are nothing new,
as they were already present on above histograms:

- Higher number of pregnancies seems to increase
the chances on developing diabetes.

- The glucose and insulin levels of the positive
population is higher in contrast to the negative one.

Looking now at the representations for the covariance matrices,
the major changes are that the correlation between skin thickness and 
diabetes pedigree function is lower in the group who do not have diabetes
than in the one having the disease.
Moreover, the correlation between BMI [^3] and age is positive 
for the sane group while negative on the positive one.

[^3]: Remember this is the logarithm.

We now search for outliers.
The idea is to use Mahalanobis distance.
As we suppose that our data $X \sim \mathcal{N}(\mu, \Sigma)$,
we have $D_M(x,\mu)^2 \sim \chi_p^2$, with $D_M$ the
Mahalanobis distance.
Hence we may set our outlier criteria as
\begin{equation}
    D_M(x, \mu)^2 > \chi_{p, 0.95^{1/n}}^2 \;,
\end{equation}
the $0.95^\text{th}$ quantile of the $\chi_p^2$ distribution.
We then drop these outliers.

```{r}
p   <- length(xnames)
n0  <- nrow(df0)
n1  <- nrow(df1)
df0_clean <- df0[mcd_neg$mah < qchisq(0.95^(1 / n0), p), ]
df1_clean <- df1[mcd_pos$mah < qchisq(0.95^(1 / n1), p), ]
df_clean  <- rbind(df0_clean, df1_clean)
```

```{r, echo = FALSE}
print(paste("The negative set contained", n0 - nrow(df0_clean), "outliers."))
print(paste("The positive set contained", n1 - nrow(df1_clean), "outliers."))
```

As a final summary of this section, we
perform a plot in which the histograms of different populations
are observed, as well as scatterplots of pairs of variables
and correlations.

```{r, fig.dim=c(16,16)}
ggpairs(df_clean, aes(color = Outcome), legend = 1,
        columns = xnames,
        diag = list(continuous = "barDiag")
        ) +
    theme(legend.position = "bottom") +
    scale_fill_manual(values = c("pink", "deeppink4")) +
    scale_color_manual(values = c("pink", "deeppink4")) + labs(fill = "Outcome")
```

<!-- TODO: PCA -->
<!-- definitive data: df_clean -->
<!-- definitive attributes: df_clean[, xnames] -->
<!-- definitive objective variable: df_clean$Outcome -->

# Supervised Classification

```{r}
color_1 <- "deepskyblue2"
color_2 <- "darkorchid4"
color_3 <- "seagreen2"
color_4 <- "indianred2"
```

We split the data into the predictors ($X$) and the variable we want to predict ($Y$):
```{r}
X <- df_clean[,xnames]
Y <- df_clean$Outcome
```



```{r}
n <- nrow(X)
p <- ncol(X)
c(n,p)
```

```{r}
n_no <- sum(Y=="Negative")
n_yes <- sum(Y=="Positive")
c(n_no,n_yes)
```

```{r}
pr_no <- n_no/n
pr_yes <- n_yes/n
c(pr_no,pr_yes)
```
To create the training and test partitions we will make a 70/30 partition, that is, 70% of the individuals will go to the training partition and the remaining 30% to the test partition.

In order to do that, first we are going to compute the number of individuals of each partition:

```{r}
n_train <- floor(.7*n) ## 70/30 partition for train and test
n_test <- n - n_train
c(n_train,n_test)
```

With that computed, we can generate the index of the individuals that are going to belong to the training partition:
```{r}
i_train <- sort(sample(1:n,n_train))
```
The individuals that are going to belong to the testing partition will be the ones that do not belong to the training partition, thus we can generate the training and testing partitions by:

```{r}
X_train <- X[i_train,]
X_test <- X[-i_train,]
Y_train <- Y[i_train]
Y_test <- Y[-i_train]
```


```{r}
np_train <- sum(Y_train=="Negative")/n_train
pp_train <- sum(Y_train=="Positive")/n_train
cat(paste0())
```

```{r}
np_test <- sum(Y_test=="Negative")/n_test
pp_train <- sum(Y_test=="Positive")/n_test
```


## Logistic Regression

The first model we are going to test is logistic regression. We start by training the model and checking the summary:
```{r}
lr_train <- multinom(Y_train~.,data=X_train)
summary(lr_train)
```

In order to see which are the most significant coefficients, we can use the t-test:

```{r}
t_test_lr_train <- summary(lr_train)$coefficients/summary(lr_train)$standard.errors
sort(abs(t_test_lr_train),decreasing=TRUE)
```
We can see that the most important variables are *Glucose*, *LogBMI*, *LogDPF* and *LogAge*.

Next, we make the predictions and check the number of instances classified in group:
```{r}
lr_test <- predict(lr_train,newdata=as.data.frame(X_test))
summary(lr_test) %>% kable() %>% kable_styling(latex_options = "striped")
```
It may be interesting to see the confusion matrix and the TER (Test Error Rate). The confussion matrix is:

```{r, echo=FALSE}
table <- data.frame("Classified_as_negative" = c("True negative (TN)", "False negative (FN)"),
                    "Classified_as_positive" = c("False positive (FP)", "True positive (TP)"))
rownames(table) <- c("Instances_actually_negative", "Instances_actually_positive")

kable(table) %>% kable_styling(latex_options = "striped")
```
With that in mind, we can define:

- $TPR=TP/(TP+FN)$ (True Positive Rate i.e. Sensitivity)
- $FNR=FN/(TP+FN)$ (False Negative Rate)
- $FPR=FP/(FP+TN)$ (False Positive Rate)
- $TNR=TN/(FP+TN)$ (True Negative Rate i.e Specificity)
- $\text{Accuracy}=(TP+TN)/(TP+TN+FP+FN)$
- $BAC=(TPR+TNR)/2$

The TER is no more than $1 - \text{Accuracy}$. In other words, the percentage of misclassified individuals.
 
We compute the confusion matrix:
```{r}
cm_lg_default <- table(Y_test,lr_test)
cm_lg_default %>% kable() %>% kable_styling(latex_options = "striped")
```
It can be observed that the model classifies well those individuals who do not have diabetes, but has a hard time classifying those individuals who have diabetes. This is because the dataset is not balanced.

Now, we compute both the TER and the accuracy:

```{r}
lr_TER <- mean(Y_test!=lr_test)
lr_TER
lr_ACC <- 1 - lr_TER
lr_ACC
```
The value obtained is not bad, but we must keep in mind that the classifier is doing really bad in classifying diabetic individuals. This can be seen more clearly if we calculate the Balanced Accuracy (BAC).

First we define a function to compute the BAC given a confusion matrix:

```{r}
bac <- function(cm){
  return(((cm[1,1]/(cm[1,1] + cm[1,2])) + (cm[2,2]/(cm[2,1] + cm[2,2])))/2)
}
```

Thus, we can compute the BAC by using it:
```{r}
bac(cm_lg_default)
```

It is observed that the value obtained is noticeably worse than the accuracy, as it balances sensitivity and specificity.

```{r}
# Obtain the probabilities of non-diabetes

prob_lr_test <- 1 - predict(lr_train,newdata=X_test,type ="probs")
head(prob_lr_test)

# Make a plot of the probabilities of being nonspam
# In green, good classifications, in red, wrong classifications

colors_errors <- c(color_4,color_3)[1*(Y_test==lr_test)+1]
plot(1:n_test,prob_lr_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of Non-Diabetes",
     main="Probabilities of Diabetes")
abline(h=0.5)
```

We are not happy with the performance of the model so we are going to try to improve it.

In order to try to improve it, we are going to first do stepwise model selection. 
The information criterion that we are going to use in order to evaluate the measure the performance of the model is the *Bayesian Information Criterion* (BIC), and it is defined as

$$
BIC = -2l(model)+npar(model)\cdot\log(n)
$$
and it aims to balance the model complexity and fitness. We do the stepwide model selection:

```{r}
### BIC
final_df <- as.data.frame(cbind(X_train, Y_train))
final_df$Y_train <- ifelse(final_df$Y_train=="Negative", 0, 1)
mod_zero  <- glm(Y_train ~ 1, family = binomial, data = final_df)
mod_all   <- glm(Y_train ~ ., family = binomial, data = final_df)
model_glm <- MASS::stepAIC(mod_zero, scope = list(lower = mod_zero, upper = mod_all), direction = "both", trace = 0, k = log(nrow(final_df)))
```

```{r}
model_glm$coefficients
```

We can see that the variables selected by the model are the same as the four more significant variables selected by the t-test. 

By predicting with the new model we obtain very similar results as the previous ones:

```{r}
X_test_2 <- X_test[, names(model_glm$coefficients)[-1]]

pred <- predict(model_glm, X_test_2, type="response") > 0.5
pred <- c("Negative", "Positive")[(1*pred)+1]
cm <- table(Y_test, pred)
cm %>% kable() %>% kable_styling(latex_options = "striped")
```

And almost the same BAC:

```{r}
BAC <- bac(cm)
BAC
```

As the logistic regression is a scoring classifier (i.e. a classifier that predicts a real value representing the probability that the instance belongs to a certain class, in our case, to be positive in diabetes) we can choose the threshold we can define the threshold at which we decide that the instance is considered to be positive for diabetes. Until now, we have been using $0.5$ as the threshold.

Note that this is one of the many options available to handle imbalanced problems. Other approaches may be oversampling or undersampling, but we have chosen to use thresholding because we are using scoring classifiers.

We define a function that takes the model, the data and a given threshold and returns the prediction given that threshold.

```{r}
get_logistic_pred = function(model, data, threshold = 0.5) {
  probs = predict(model,newdata=data,type="response")
  ifelse(probs > threshold, "Positive", "Negative")
}
```

Thus, we can compute the the new predictions for different cuts ($0.1$, $0.5$ and $0.9$) by:
```{r}
test_pred_10_lg = get_logistic_pred(model_glm, data = X_test_2,
                                 threshold = 0.1)
test_pred_50_lg = get_logistic_pred(model_glm, data = X_test_2, 
                                 threshold = 0.5)
test_pred_90_lg = get_logistic_pred(model_glm, data = X_test_2,
                                 threshold = 0.9)
```
For these thresholds, we now can compute they accuracy, sensitivity and specificity:
```{r}
test_tab_10_lg = table(predicted = test_pred_10_lg, actual = Y_test)
test_tab_50_lg = table(predicted = test_pred_50_lg, actual = Y_test)
test_tab_90_lg = table(predicted = test_pred_90_lg, actual = Y_test)

test_con_mat_10_lg = confusionMatrix(test_tab_10_lg, positive = "Positive")
test_con_mat_50_lg = confusionMatrix(test_tab_50_lg, positive = "Positive")
test_con_mat_90_lg = confusionMatrix(test_tab_90_lg, positive = "Positive")

metrics = rbind(
  c(test_con_mat_10_lg$overall["Accuracy"], 
    test_con_mat_10_lg$byClass["Sensitivity"], 
    test_con_mat_10_lg$byClass["Specificity"]),
  
  c(test_con_mat_50_lg$overall["Accuracy"], 
    test_con_mat_50_lg$byClass["Sensitivity"], 
    test_con_mat_50_lg$byClass["Specificity"]),
  
  c(test_con_mat_90_lg$overall["Accuracy"], 
    test_con_mat_90_lg$byClass["Sensitivity"], 
    test_con_mat_90_lg$byClass["Specificity"])
  
)
rownames(metrics) = c("c = 0.10", "c = 0.50", "c = 0.90")
colnames(metrics) = c("Accuracy", "Sensitivity", "Specificity")

metrics %>% kable() %>% kable_styling(latex_options = "striped")
```
As can be seen, with the threshold=$0.1$ we obtain a high sensitivity but a low specificity and, for the threshold=$0.9$ a low sensitivity but a low specificity. We can plot a ROC curve to search for the best threshold.
```{r}
test_prob = predict(model_glm, newdata = X_test_2, type = "response")

ROC_lr = plot.roc(Y_test, test_prob,
         main="Confidence interval of a threshold", percent=TRUE,
         ci=TRUE, of="thresholds",
         thresholds="best", 
         print.thres="best",
         print.auc=TRUE) 
```
It is around $0.4$
```{r}
ROC_lr
```

If we compute the predictions with the new threshold:

```{r}
pred <- predict(model_glm, X_test_2, type = "response") > 
  as.numeric(rownames(ROC_lr$ci$specificity))
pred <- pred * 1
pred <- as.factor(c("Negative", "Positive")[(1*pred)+1])
cm <- table(Y_test, pred)
cm %>% kable() %>% kable_styling(latex_options = "striped")
```

We can see a much better confussion matrix, and, if we compute de BAC:

```{r}
BAC <- bac(cm)
BAC
```
It has improven a quite a bit.
```{r}
colors_errors <- c(color_4,color_3)[1*(Y_test==pred)+1]
plot(1:n_test,prob_lr_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of Diabetes",
     main="Probabilities of Diabetes")
abline(h=as.numeric(rownames(ROC_lr$ci$specificity)))
```



## Methods based on the Bayes Theorem

We are going to try three methods based on the Bayes Theorem:

- Linear discriminant analysis
- Quadratic discriminant analysis
- Naive Bayes

These methods estimate the prior probabilities as the proportion of available observations belonging to each class and they make predictions selecting the class for which the posterior probability becomes maximum.

As they take into account the prior probabilities, these methods are very convenient for unbalanced problems (as the problem we are facing), so minimal tuning may be needed.

### Linear discriminant analysis (LDA)

First, we train the model by:

```{r}
lda_train <- lda(Y_train~.,data=X_train)
```
As it is a bayesian method, it consider prior probabilities so it is taking into account that the dataset is balanced. The prior probabilities are:

```{r}
lda_train$prior
```
that are the same as the proportion of people with and without diabetes in our dataset:
```{r}
c(pr_no, pr_yes)
```

With the model fitted, we can make the predictions as:

```{r}
lda_test <- predict(lda_train,newdata=X_test)
```

And save the predicted classes as:

```{r}
lda_Y_test <- lda_test$class
```

And the confusion table is:

```{r}
cm_LDA <- confusionMatrix(table(Y_test,lda_Y_test), positive="Positive")
cm_LDA$table %>% kable() %>% kable_styling(latex_options = "striped")
```


```{r}
lda_TER <- mean(Y_test!=lda_Y_test)
lda_TER
lda_accuracy <- 1 - lda_TER
lda_accuracy
```
Computing the BAC we can see that it is much better than the default logistic regression. The reason behind this is that, as said earlier, due to its bayesian nature, it takes into account that the problem is unbalanced.
If we compute its sensitivity and specificity:

```{r}
test_tab_50_lda = table(predicted = lda_Y_test, actual = Y_test)
test_con_mat_50_lda = confusionMatrix(test_tab_50_lda, positive = "Positive")

metrics = rbind(

  c( 
    test_con_mat_50_lda$byClass["Sensitivity"], 
    test_con_mat_50_lda$byClass["Specificity"])
)
rownames(metrics) = c("c = 0.50")
colnames(metrics) = c("Sensitivity", "Specificity")
metrics
```
It can be seen that the specificity is much larger than the sensitivity. We can also compute the BAC by:

```{r}
bac(cm_LDA$table)
```


Recall can obtain the conditional probabilities of the classifications made with the test sample:
```{r}
prob_lda_Y_test <- lda_test$posterior
head(prob_lda_Y_test)
```

Thanks to this, we can treat it as a scoring classifier. In order to find the optimum thresold, we can plot a roc curve:

```{r}
test_prob = predict(lda_train, newdata = X_test, type = "response")
test_prob <- test_prob$posterior[,2]

ROC_lda = plot.roc(Y_test, test_prob,
               main="Confidence interval of a threshold", percent=TRUE,
               ci=TRUE, of="thresholds", 
               thresholds="best", 
               print.thres="best",
               print.auc=TRUE) 
```

It seems that, according to the ROC curve, the optimum threshold is:
```{r}
as.numeric(rownames(ROC_lda$ci$specificity))
```


If we compute the predictions with the new threshold:

```{r}
pred <- predict(lda_train, X_test, type = "response")$posterior[,2] > 
  as.numeric(rownames(ROC_lda$ci$specificity))
pred <- pred * 1
pred <- as.factor(c("Negative", "Positive")[(1*pred)+1])
cm_lda_opt <- table(Y_test, pred)
cm_lda_opt %>% kable() %>% kable_styling(latex_options = "striped")
```

We can also compute the BAC:

```{r}
BAC <- bac(cm)
BAC
```

It is similar to the BAC computed in the logistic regression after tuning the threshold.

Finally, we plot the probability of being positive of diabetes and the threshold:
```{r}
probs <- predict(lda_train, X_test, type = "response")$posterior[,2]
colors_errors <- c(color_4,color_3)[1*(Y_test==pred)+1]
plot(1:n_test,probs,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of Diabetes",
     main="Probabilities of Diabetes")
abline(h=as.numeric(rownames(ROC_lda$ci$specificity)))
```


### Quadraric discriminant analysis (QDA)

First, we train the model by:

```{r}
qda_train <- qda(Y_train~.,data=X_train)
```
The workflow for this method will be mostly the same as with the LDA.


We make the predictions as:

```{r}
qda_test <- predict(lda_train,newdata=X_test)
```

We can also save the predictions as:

```{r}
qda_Y_test <- lda_test$class
```

And the confusion table is:

```{r}
cm_QDA <- confusionMatrix(table(Y_test,qda_Y_test), positive="Positive")
cm_QDA$table %>% kable() %>% kable_styling(latex_options = "striped")
```
This is the same confusion table as for the default LDA! This means that accuracy, sensitivity, specificity... will be all the same.


Finally, we plot the probabilities of having diabetes and the default threshold.

```{r}
probs <- predict(qda_train, X_test, type = "response")$posterior[,2]
colors_errors <- c(color_4,color_3)[1*(Y_test==qda_Y_test)+1]
plot(1:n_test,probs,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of Diabetes",
     main="Probabilities of Diabetes")
abline(h=0.5)
```



### Naive Bayes (NB)

We start by training the naive-bayes model:

```{r}
nb_train <- gaussian_naive_bayes(X_train,Y_train)
```

And computing the predictions:

```{r}
nb_test <- predict(nb_train,newdata=as.matrix(X_test),type="prob")
```

Next, we generate the vector of classifications:
```{r}
nb_Y_test <- as.factor(ifelse(nb_test[,2]>0.5, "Positive", "Negative"))
```

By computing the confusion matrix, we can see that the method yields good results:

```{r}
cm_nb <- confusionMatrix(table(Y_test,nb_Y_test), positive = "Positive")
cm_nb$table
```


```{r}
nb_TER <- mean(Y_test!=nb_Y_test)
print(paste0("Test Error Rate: ",nb_TER))
nb_acc <- 1 - nb_TER
print(paste0("Accuracy: ",nb_acc))
cm_nb$byClass["Sensitivity"]
cm_nb$byClass["Specificity"]
```


```{r}
test_prob <-  predict(nb_train, newdata = as.matrix(X_test), type = "prob")
prob_nb_Y_test <- test_prob[,2]
colors_errors <- c(color_4,color_3)[1*(Y_test==nb_Y_test)+1]
plot(1:n_test,prob_nb_Y_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of non diabetes",
     main="Probabilities of non diabetes")
abline(h=0.5)
```


```{r}
ROC_nb = plot.roc(Y_test, prob_nb_Y_test,
               main="Confidence interval of a threshold", percent=TRUE,
               ci=TRUE, of="thresholds",
               thresholds="best", 
               print.thres="best",
               print.auc=TRUE) 
ROC_nb
```

```{r}
nb_Y_test_tuned <- as.factor(ifelse(nb_test[,2]>as.numeric(rownames(ROC_lda$ci$specificity)), "Positive", "Negative"))
cm_nb <- confusionMatrix(table(Y_test,nb_Y_test_tuned), positive = "Positive")
cm_nb$table
```


## KNN (k-nearest neighbors)

The last supervised classification method that we are going to test is KNN.

First, we are going to find the best $K$ value using cross-validation.

```{r}
LER <- rep(NA,20)
for (i in 3 : 20){
  knn_output <- knn.cv(X_train,Y_train,k=i)
  LER[i] <- 1 - mean(knn_output==Y_train)
}
LER
plot(1:20,LER,pch=20,col=color_1,type="b",
     xlab="k",ylab="LER",main="LER for the diabetes dataset")
```

It seems that the optimal $K$ is

```{r}
k <- which.min(LER)
k
```


```{r}
knn_Y_test <- knn(X_train,X_test,Y_train,k=k,prob=T)
table(Y_test,knn_Y_test)
```

We compute the TER by:

```{r}
knn_TER <- mean(Y_test!=knn_Y_test)
knn_TER
```


```{r}
prob_knn_Y_test <- ifelse(knn_Y_test == "Positive", attributes(knn_Y_test)$prob, 1 - attributes(knn_Y_test)$prob)
```


```{r}
head(prob_knn_Y_test)
prob <- 2*ifelse(knn_Y_test == "-1", prob_knn_Y_test, 1-prob_knn_Y_test) - 1
```



```{r}
# Make a plot of the probabilities of the winner group
# In green, good classifications, in red, wrong classifications

colors_errors <- c(color_4,color_3)[1*(Y_test==knn_Y_test)+1]
plot(1:n_test,prob_knn_Y_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Diabetes probabilities",main="Diabetes probabilities")
```



## Conclusions of the section

We have tried five different supervised classification methods:

- Logistic regression (LR)
- Linear discriminant analysis (LDA)
- Quadratic discriminant analysis (QDA)
- Naive Bayes (NB)
- K-Nearest Neighbors (KNN)

As we were faced with an unbalanced problem, it was not enough to look at metrics such as accuracy, and we had to consult others such as balanced accuracy, sensitivity or specificity. According to these metrics, the default LR did quite poorly, but after thresholding with the help of the ROC curve, we got really good results.

On the other hand, and as we have commented throughout this section, Bayesian methods have the advantage in this type of problem that they take into account the a priori probability, so that by default they already consider that the problem is unbalanced. Thanks to this, they achieve very good results by default, but, depending on the levels of sensitivity and specificity to be achieved, it may be a good idea to perform thresholding.

We believe that considering what levels of sensitivity and specificity we want to obtain is especially important in medical problems (such as the one we are facing), since, for certain problems, we may be especially interested in correctly classifying positive individuals (Sensitivity), in others negative individuals (Specificity), or in others in finding a balance (Balanced Accuracy).

Sobre el KNN no tengo nada que decir XD





