---
title: "Team Project"
output: pdf_document
author: 
  - José Ignacio Díez Ruiz	100487766
  - Carlos Roldán Piñero	100484904
  - Pablo Vidal Fernández	100483812
date: "`r Sys.Date()`"
header-includes:
  - \renewcommand{\and}{\\}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
require(ggplot2)
require(tidyverse)
require(mice)
require(visdat)
require(MASS)
require(pracma)
require(rrcov)
require(corrplot)
require(GGally)
require(class)
require(moments)
require(nnet)
require(naivebayes)
require(pROC)
require(caret)
```

<!-- TODO: Introduction -->

# Descriptive Analysis and Preprocessing

## Reading the data and setting the NAs

Before starting the analysis, we need to make some preprocessing on our data.
Let us start by loading it into memory and listing the names of the columns.

```{r}
data <- read.csv("diabetes.csv")
colnames(data)
```

Of these, our target variable is `Outcome`, which has two levels.
For convenience, we transform it into a factor variable which R can trat accordingly.

```{r}
data$Outcome <- factor(data$Outcome, c(0, 1), c("Negative", "Positive"))
```

<!-- Up to this point this would go in the introduction -->

Now we need to address a particularity of the chosen data:
not-a-number (NaN) instances are encoded as zeros in variables
where that value is imposible [^1].
These are:

- Glucose

- BloodPressure

- SkinThickness

- Insulin

- BMI

In order for us to later trat them correctly, we need to manually
change them to the existing `NA` type.
As we do so, we record the number of NaNs instances in each of those variables.
For convenience, we define a function.

[^1]: Remember we are dealing with medical data, not with artificial one.
Hence, there are constraints on the values a variable may take.

```{r}
set_nas <- function(data, fields) {
    percentage <- list()
    for (field in fields) {
        data[[field]][data[[field]] == 0]   <- NA
        percentage[[field]] <- 100 * sum(is.na(data[[field]])) / nrow(data)
    }
    return(list(data = data, percentage = percentage))
}

# Correctly label NaNs
na_fields   <- c("Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI")
data_na     <- set_nas(data, na_fields)
data        <- data_na$data
percentages <- data_na$percentage

# Visualize them
vis_miss(data)
```

Now the next logical step is to impute those NaN values.
We do have some concern about the imputation of the "Insulin" variable
which is almost half-filled of NaNs.
Nevertheless, we decide to impute it.
On the followed strategy, we use the "Predictive Mean Matching Imputation"
(PMM in short) as it behaves much more robustly than naive
mean or median imputations.

```{r}
data_im <- mice(data, m = 1, method = "pmm")
data    <- complete(data_im)
```

## Visualization of the data

Before we proceed any further, we are going to describe our data.
First, we look individually to each of the attributes,
we see both visually and with with a two-sample Wilcox-test whether there
is significant difference between the two groups (having or not diabetes),
and if some transformation may be desirable to ensure normality compatibility.
For that purpose, we define the following function.

```{r}
histogram_by_groups <- function(data, var, label = NULL) {
    stat_t <- wilcox.test(as.formula(paste(var, "~ Outcome")), data)
    data0  <- data[data$Outcome == "Negative", ]
    data1  <- data[data$Outcome == "Positive", ]
    if (is.null(label)) {
        label <- var
    }
    p <- ggplot(data0, aes(x = eval(parse(text = var)))) +
        geom_histogram(
            aes(y = after_stat(count / sum(count)), fill = "Negative"),
            bins = 10, colour = "white", alpha = 0.8, boundary = 0
            ) +
        geom_histogram(data = data1,
            aes(
                x = eval(parse(text = var)),
                y = after_stat(count / sum(count)), fill = "Positive"
                ),
            bins = 10, colour = "white",
            alpha = 0.5, boundary = 0, inherit.aes = FALSE) +
            theme_bw() +
            scale_fill_manual(
                name = "",
                breaks = c("Positive", "Negative"),
                values = c("Positive" = "deeppink4", "Negative" = "pink2")
            ) +
            xlab(label) + ylab("Relative frequency") + ggtitle(label) +
            geom_vline(xintercept = mean(data1[[var]]), colour = "deeppink4") +
            geom_vline(xintercept = mean(data0[[var]]), colour = "pink2")
    p + annotate(
            "text",
            x = 0.9 * max(data1[var]),
            y = 0.9 * max(ggplot_build(p)$data[[1]]["y"]),
            label = sprintf("p-value = %.4e", stat_t$p.value),
            size = 3
        )
}
```

Let us start with the number of pregnancies.
We can see that people who have diabetes have had more
pregnancies than those who do not have diabetes.
We see that it seems to be somewhat based on the p-value alone.
We also note it exhibits a heavily right-skewed behaviour.
As such, a logarithmic transformation would make sense to get
a distribution more compatible with the normal one.
Nonetheless, a problem here arises in dealing with the null values [^2].
For that purpose we shift the variable by one unit.
As a consistency measure, we will apply this shift to all
variables we log-transform.

[^2]: Remember the domain of the logarithmic function is $(0, \infty)$.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Pregnancies")
```

The change as we see does help in obtaining a more
centered distribution with which we may better apply
the posterior analysis.

```{r, fig.dim = c(8, 4)}
data$LogPregnancies <- log(data$Pregnancies + 1)
histogram_by_groups(data, "LogPregnancies")
```

The next variable to visualize is the glucose.
People with diabetes exhibit higher glucose values.
The p-value  is very small which
indicates a highly significant difference between the two groups.
We also observe that the distribution is already
well-centered and resembles a normal distribution.
Hence, we decide not to transform the data.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Glucose")
```

We move onwards to blood pressure.
In this case, although the distributions appear
as normal, there does not seem to exist a highly significant
difference between the groups in contrast to what the p-value states,
more so compared with the glucose variable.
Nonetheless, there seem to be an slight indication
of higher blood pressure for people with diabetes.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "BloodPressure")
```

Now skin thickness is interesting, because the distributions
are visually to those of the blood pressure but the presence
of outliers is appreciable.
We will later deal with those but for now let us
mantain this variable as it is.
Note that the population with diabetes appear
to exhibit a thicker skin.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "SkinThickness")
```

Insulin, as we may have expected from the name of the property
itself, appears to be a relevant.
The median of the distributions does indeed seem to differ,
with the one for the diabetes population being
slightly higher.
It is also right-skewed, so we decide to log-transform it.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Insulin")
```

It does appear that the transformation impproves the
the symmetrization of the data, although some left-skewness
appears.
We will later see if outlier detection get to
target those values or not.

```{r, fig.dim = c(8, 4)}
data$LogInsulin <- log(data$Insulin + 1)
histogram_by_groups(data, "LogInsulin")
```

Body Mass Index (BMI) again exhibits this tendency of
leaning towards a more right-skewed distribution.
It does also follow the tendency of being
slightly higher for people with diabetes.
As such we log-transform to try and get a more normalized variable.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "BMI")
```

As we may visually judge, it is the case
that the log transformation centers the data
and provides a more normal-like distribution.
There is some presence of seemingly outlying points
to the right.

```{r, fig.dim = c(8, 4)}
data$LogBMI <- log(data$BMI + 1)
histogram_by_groups(data, "LogBMI")
```

The Diabetes Pedigree Function (DPF) is the
again a flagrant right-skewed.
It again has higher values for the positive set.
This is to be expected from the definition of this
very function as a risk indication for diabetes.
Let us try to log-transform it.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "DiabetesPedigreeFunction", "DPF")
```

The improvement is highly noticeable.
We retain thus this transformed variable.

```{r, fig.dim = c(8, 4)}
data$LogDPF <- log(data$DiabetesPedigreeFunction + 1)
histogram_by_groups(data, "LogDPF")
```

We may note that young people, as with other illnesses
have less tendency to suffer diabetes than the elders.
The age is expected to exhibit a right-skewed
distribution, as is indeed the case.
In an attempt to improve the symmetry,
we once again use logarithms to transform the variable.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Age")
```

The transformation does help although
not by much.
This is a common problem of the age variable.
We keep the transformation anyway as it does seem
to help with the symmetry for the positive group.

```{r, fig.dim = c(8, 4)}
data$LogAge <- log(data$Age + 1)
histogram_by_groups(data, "LogAge")
```

```{r}
# Some convenience
df  <- subset(data, select = -c(Pregnancies, Insulin, BMI, DiabetesPedigreeFunction, Age))
df0     <-  df[df$Outcome == "Negative", ]
df1     <-  df[df$Outcome == "Positive", ]
xnames  <- names(df)[! names(df) %in% c("Outcome")]
```

Now, let's take a look at some multivariate plots. We'll begin by inspecting the Parallel Coordinate Plot:

```{r, fig.dim = c(8, 4)}
par(las = 2)
parcoord(df[xnames], col = c("pink2", "deeppink4"))
legend("topright", legend = c("Negative", "Positive"),
       col = c("pink2", "deeppink4"), lty = 1, lwd = 2)
```

It seems that, overall, the positive lines are 
over negative ones. 
This is most notable on the Glucose and log
transformed BMI.
They are two of the most significant
features according to the p-value.

The Andrew's plot is the following: 

```{r, fig.dim = c(8, 4)}
andrewsplot(as.matrix(df[xnames]), df$Outcome, style = "cart")
legend("topright", legend = c("Negative", "Positive"),
       col = c("black", "red"), lty = 1, lwd = 2)
```

Again, we see that the two groups are different.
The group of people who have diabetes tend to have more volatile curves,
reaching higher and lower values along the curve.

## Multivariate characteristics and outlier identification

We are goig to use a multivariate approach
to identifying the outliers in our data.
In the process, we will need to compute the
mean and covariance.

We start then by finding the mean vector and the
covariance matrix.
In order to reduce the sensitivity to outliers in this
computation, we use a robust estimation using 
the "Fast MCD" (Minimum Covariance Determinant) estimator.
We set main parameter, `alpha`, which determines the percentage
of the data to use, at $0.85$.

<!-- TODO: refactor the prints -->

```{r}
our_corrplot <- function(cov_mat) {
    colnames(cov_mat) <- c("Log\nPregnancies",
                           "Glucose",
                           "Blood\nPressure",
                           "Skin\nThickness",
                           "LogInsulin",
                           "LogBMI",
                           "LogDPF",
                           "LogAge")
    corrplot.mixed(cov2cor(cov_mat), lower = "number", upper = "color",
        diag = "n", tl.col = "black", tl.cex = 0.65,
        lower.col = "black")
}

mcd_est <- CovMcd(df[xnames], alpha = 0.85, nsamp = "deterministic")
```

```{r, echo = FALSE, fig.size = c(8,8)}
print("The mean vector is:")
print(mcd_est$center)
print("The covariance matrix is:")
print(round(mcd_est$cov, 4))
```

```{r, fig.size = c(8,8)}
our_corrplot(mcd_est$cov)
```

It is interesting to seggregate by the class
of the Outcome variable.
This way, we can both, get the mean vector and covariance
matrix for each of the classes, and the outliers for both sets.

```{r, fig.size = c(8,8)}
mcd_neg <- CovMcd(df0[xnames], alpha = 0.85, nsamp = "deterministic")
mcd_pos <- CovMcd(df1[xnames], alpha = 0.85, nsamp = "deterministic")
```

```{r, echo = FALSE, fig.size = c(8,8)}
print("### Negative class ###")
print("The mean vector is:")
print(mcd_neg$center)
print("The covariance matrix is:")
print(round(mcd_neg$cov, 4))
```

```{r, fig.size = c(8,8)}
our_corrplot(mcd_neg$cov)
```

```{r, echo = FALSE, fig.size = c(8,8)}
print("### Positive class ###")
print("The mean vector is:")
print(mcd_pos$center)
print("The covariance matrix is:")
print(round(mcd_pos$cov, 4))
```

```{r, fig.size = c(8,8)}
our_corrplot(mcd_pos$cov)
```

Let us focus first on the mean vectors.
The here observed particularities are nothing new,
as they were already present on above histograms:

- Higher number of pregnancies seems to increase
the chances on developing diabetes.

- The glucose and insulin levels of the positive
population is higher in contrast to the negative one.

Looking now at the representations for the covariance matrices,
the major changes are that the correlation between skin thickness and 
diabetes pedigree function is lower in the group who do not have diabetes
than in the one having the disease.
Moreover, the correlation between BMI [^3] and age is positive 
for the sane group while negative on the positive one.

[^3]: Remember this is the logarithm.

We now search for outliers.
The idea is to use Mahalanobis distance.
As we suppose that our data $X \sim \mathcal{N}(\mu, \Sigma)$,
we have $D_M(x,\mu)^2 \sim \chi_p^2$, with $D_M$ the
Mahalanobis distance.
Hence we may set our outlier criteria as
\begin{equation}
    D_M(x, \mu)^2 > \chi_{p, 0.95^{1/n}}^2 \;,
\end{equation}
the $0.95^\text{th}$ quantile of the $\chi_p^2$ distribution.
We then drop these outliers.

```{r}
p   <- length(xnames)
n0  <- nrow(df0)
n1  <- nrow(df1)
df0_clean <- df0[mcd_neg$mah < qchisq(0.95^(1 / n0), p), ]
df1_clean <- df1[mcd_pos$mah < qchisq(0.95^(1 / n1), p), ]
df_clean  <- rbind(df0_clean, df1_clean)
```

```{r, echo = FALSE}
print(paste("The negative set contained", n0 - nrow(df0_clean), "outliers."))
print(paste("The positive set contained", n1 - nrow(df1_clean), "outliers."))
```

As a final summary of this section, we
perform a plot in which the histograms of different populations
are observed, as well as scatterplots of pairs of variables
and correlations.

```{r, fig.dim=c(16,16)}
ggpairs(df_clean, aes(color = Outcome), legend = 1,
        columns = xnames,
        diag = list(continuous = "barDiag")
        ) +
    theme(legend.position = "bottom") +
    scale_fill_manual(values = c("pink", "deeppink4")) +
    scale_color_manual(values = c("pink", "deeppink4")) + labs(fill = "Outcome")
```

<!-- TODO: PCA -->
<!-- definitive data: df_clean -->
<!-- definitive attributes: df_clean[, xnames] -->
<!-- definitive objective variable: df_clean$Outcome -->

# Supervised Classification

```{r}
color_1 <- "deepskyblue2"
color_2 <- "darkorchid4"
color_3 <- "seagreen2"
color_4 <- "indianred2"
```

We split the data into the predictors ($X$) and the variable we want to predict ($Y$):
```{r}
X <- df_clean[,xnames]
Y <- df_clean$Outcome
```



```{r}
n <- nrow(X)
p <- ncol(X)
c(n,p)
```

```{r}
n_no <- sum(Y=="Negative")
n_yes <- sum(Y=="Positive")
c(n_no,n_yes)
```

```{r}
pr_no <- n_no/n
pr_yes <- n_yes/n
c(pr_no,pr_yes)
```
To create the training and test partitions we will make a 70/30 partition, that is, 70% of the individuals will go to the training partition and the remaining 30% to the test partition.

In order to do that, first we are going to compute the number of individuals of each partition:

```{r}
n_train <- floor(.7*n) ## 70/30 partition for train and test
n_test <- n - n_train
c(n_train,n_test)
```

With that computed, we can generate the index of the individuals that are going to belong to the training partition:
```{r}
i_train <- sort(sample(1:n,n_train))
```
The individuals that are going to belong to the testing partition will be the ones that do not belong to the training partition, thus we can generate the training and testing partitions by:

```{r}
X_train <- X[i_train,]
X_test <- X[-i_train,]
Y_train <- Y[i_train]
Y_test <- Y[-i_train]
```


```{r}
np_train <- sum(Y_train=="Negative")/n_train
pp_train <- sum(Y_train=="Positive")/n_train
cat(paste0())
```

```{r}
np_test <- sum(Y_test=="Negative")/n_test
pp_train <- sum(Y_test=="Positive")/n_test
```


## Logistic Regression

The first model we are going to test is logistic regression. We start by training the model and checking the summary:
```{r}
lr_train <- multinom(Y_train~.,data=X_train)
summary(lr_train)
```

In order to see which are the most significant coefficients, we can use the t-test:

```{r}
t_test_lr_train <- summary(lr_train)$coefficients/summary(lr_train)$standard.errors
sort(abs(t_test_lr_train),decreasing=TRUE)
```
We can see that the most important variables are *Glucose*, *LogBMI*, *LogDPF* and *LogAge*.

Next, we make the predictions and check the number of instances classified in group:
```{r}
lr_test <- predict(lr_train,newdata=as.data.frame(X_test))
summary(lr_test)
```
It may be interesting to see the confusion matrix and the TER (Test Error Rate), which is no more than 1 - Accuracy. In other words, the percentage of missclassified individuals. First we compute the confussion matrix:
```{r}
cm_lg_default <- table(Y_test,lr_test)
cm_lg_default
```
It can be observed that the model classifies well those individuals who do not have diabetes, but has a hard time classifying those individuals who have diabetes, probably because the dataset is not balanced.

Now, we compute both the TER and the accuracy:

```{r}
lr_TER <- mean(Y_test!=lr_test)
lr_TER
lr_ACC <- 1 - lr_TER
lr_ACC
```
The value obtained is not bad, but we must keep in mind that the classifier is doing really bad in classifying diabetic individuals. This can be seen more clearly if we calculate the Balanced Accuracy (BAC).

First we define a function to compute the BAC given a confussion matrix:

```{r}
bac <- function(cm){
  return(((cm[1,1]/(cm[1,1] + cm[1,2])) + (cm[2,2]/(cm[2,1] + cm[2,2])))/2)
}
```

Thus, we can compute the BAC by using it:
```{r}
bac(cm_lg_default)
```

It is observed that the value obtained is noticeably worse than the accuracy.

```{r}
# Obtain the probabilities of non-diabetes

prob_lr_test <- 1 - predict(lr_train,newdata=X_test,type ="probs")
head(prob_lr_test)

# Make a plot of the probabilities of being nonspam
# In green, good classifications, in red, wrong classifications

colors_errors <- c(color_4,color_3)[1*(Y_test==lr_test)+1]
plot(1:n_test,prob_lr_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of Non-Diabetes",
     main="Probabilities of Diabetes")
abline(h=0.5)
```

We are not happy with the performance of the model so we are going to try to improve it.

First, we are going to do stepwise model selection. 
The information criterion that we are going to use in order to evaluate the measure the performance of the model is the *Bayesian Information Criterion* (BIC), and it is defined as

$$
BIC = -2l(model)+npar(model)\cdot\log(n)
$$
and it aims to balance the model complexity and fitness.

```{r}
### BIC
final_df <- as.data.frame(cbind(X_train, Y_train))
final_df$Y_train <- ifelse(final_df$Y_train=="Negative", 0, 1)
mod_zero  <- glm(Y_train ~ 1, family = binomial, data = final_df)
mod_all   <- glm(Y_train ~ ., family = binomial, data = final_df)
model_glm <- MASS::stepAIC(mod_zero, scope = list(lower = mod_zero, upper = mod_all), direction = "both", trace = 0, k = log(nrow(final_df)))
```

```{r}
model_glm$coefficients
```

We can see that the variables selected by the model are the same as the four more significant variables selected by the t-test

```{r}
X_test_2 <- X_test[, names(model_glm$coefficients)[-1]]

pred <- predict(model_glm, X_test_2, type="response") > 0.5
pred <- c("Negative", "Positive")[(1*pred)+1]
cm <- table(Y_test, pred)
cm
```


```{r}
BAC <- bac(cm)
BAC
```

```{r}
##### voy a hacer una ROC
get_logistic_pred = function(mod, data, res = "y", pos = "Positive", neg = "Negative", cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  ifelse(probs > cut, pos, neg)
}

X_test_2 <- as.data.frame(X_test[, names(model_glm$coefficients)[-1]])

test_pred_10 = get_logistic_pred(model_glm, data = X_test_2, res = "default", 
                                 cut = 0.1)
test_pred_50 = get_logistic_pred(model_glm, data = X_test_2, res = "default", 
                                 cut = 0.5)
test_pred_90 = get_logistic_pred(model_glm, data = X_test_2, res = "default", 
                                 cut = 0.9)
```

```{r}
test_tab_10 = table(predicted = test_pred_10, actual = Y_test)
test_tab_50 = table(predicted = test_pred_50, actual = Y_test)
test_tab_90 = table(predicted = test_pred_90, actual = Y_test)

test_con_mat_10 = confusionMatrix(test_tab_10, positive = "Positive")
test_con_mat_50 = confusionMatrix(test_tab_50, positive = "Positive")
test_con_mat_90 = confusionMatrix(test_tab_90, positive = "Positive")

metrics = rbind(
  
  c(test_con_mat_10$overall["Accuracy"], 
    test_con_mat_10$byClass["Sensitivity"], 
    test_con_mat_10$byClass["Specificity"]),
  
  c(test_con_mat_50$overall["Accuracy"], 
    test_con_mat_50$byClass["Sensitivity"], 
    test_con_mat_50$byClass["Specificity"]),
  
  c(test_con_mat_90$overall["Accuracy"], 
    test_con_mat_90$byClass["Sensitivity"], 
    test_con_mat_90$byClass["Specificity"])
  
)
```

```{r}
rownames(metrics) = c("c = 0.10", "c = 0.50", "c = 0.90")
colnames(metrics) = c("Accuracy", "Sensitivity", "Specificity")

metrics

test_prob = predict(model_glm, newdata = X_test_2, type = "response")

ROC = plot.roc(Y_test, test_prob,
         main="Confidence interval of a threshold", percent=TRUE,
         ci=TRUE, of="thresholds", # compute AUC (of threshold)
         thresholds="best", # select the (best) threshold
         print.thres="best",
         print.auc=TRUE) # also highlight this threshold on the plot
ROC
```

```{r}
pred <- predict(model_glm, X_test_2, type = "response") > 
  as.numeric(rownames(ROC$ci$specificity))
pred <- pred * 1
cm <- table(Y_test, pred)

BAC <- bac(cm)
BAC
```



## Methods based on the Bayes Theorem

### Linear discriminant dnalysis
### Quadraric discriminant analysis
### Naive Bayes

## KNN (k-nearest neighbors)











