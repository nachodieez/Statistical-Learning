---
title: "Team Project"
output: pdf_document
author: 
  - José Ignacio Díez Ruiz	100487766
  - Carlos Roldán Piñero	100484904
  - Pablo Vidal Fernández	100483812
date: "`r Sys.Date()`"
header-includes:
  - \renewcommand{\and}{\\}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
require(ggplot2)
require(tidyverse)
require(mice)
require(visdat)
require(MASS)
require(pracma)
require(rrcov)
require(corrplot)
require(GGally)
require(factoextra)
require(ica)
require(psych)
require(class)
require(moments)
require(nnet)
require(naivebayes)
require(pROC)
require(caret)
require(kableExtra)
require(dbscan)
require(cluster)
require(mclust)
require(corpcor)
require(RSpectra)
require(factoextra)
require(knitr)
require(ica)
```

```{r, echo = F}
c1 <- "deeppink4"
c2 <- "skyblue2"
c3 <- "darkorchid4"
c4 <- "seagreen2"
set.seed(100487766)
```

<!-- TODO: Introduction -->

# Descriptive Analysis and Preprocessing

## Reading the data and setting the NAs

Before starting the analysis, we need to make some preprocessing on our data.
Let us start by loading it into memory and listing the names of the columns.

```{r}
data <- read.csv("diabetes.csv")
colnames(data)
```

Of these, our target variable is `Outcome`, which has two levels.
For convenience, we transform it into a factor variable which R can trat accordingly.

```{r}
data$Outcome <- factor(data$Outcome, c(0, 1), c("Negative", "Positive"))
```

<!-- Up to this point this would go in the introduction -->

Now we need to address a particularity of the chosen data:
not-a-number (NaN) instances are encoded as zeros in variables
where that value is imposible [^1].
These are:

- Glucose

- BloodPressure

- SkinThickness

- Insulin

- BMI

In order for us to later trat them correctly, we need to manually
change them to the existing `NA` type.
As we do so, we record the number of NaNs instances in each of those variables.
For convenience, we define a function.

[^1]: Remember we are dealing with medical data, not with artificial one.
Hence, there are constraints on the values a variable may take.

```{r}
set_nas <- function(data, fields) {
    percentage <- list()
    for (field in fields) {
        data[[field]][data[[field]] == 0]   <- NA
        percentage[[field]] <- 100 * sum(is.na(data[[field]])) / nrow(data)
    }
    return(list(data = data, percentage = percentage))
}

# Correctly label NaNs
na_fields   <- c("Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI")
data_na     <- set_nas(data, na_fields)
data        <- data_na$data
percentages <- data_na$percentage

# Visualize them
vis_miss(data)
```

Now the next logical step is to impute those NaN values.
We do have some concern about the imputation of the "Insulin" variable
which is almost half-filled of NaNs.
Nevertheless, we decide to impute it.
On the followed strategy, we use the "Predictive Mean Matching Imputation"
(PMM in short) as it behaves much more robustly than naive
mean or median imputations.

```{r}
pred_mat <- matrix(1, ncol(data), ncol(data), dimnames = list('row' = colnames(data), 'col' = colnames(data)))
pred_mat[,9] <- 0
for (i in 1:8){
  pred_mat[i,i]<-0
}
data_im <- mice(data, m = 1, method = "pmm", predictorMatrix = pred_mat)
data    <- complete(data_im)
```

## Visualization of the data

Before we proceed any further, we are going to describe our data.
First, we look individually to each of the attributes,
we see both visually and with with a two-sample Wilcox-test whether there
is significant difference between the two groups (having or not diabetes),
and if some transformation may be desirable to ensure normality compatibility.
For that purpose, we define the following function.

```{r}
histogram_by_groups <- function(data, var, label = NULL) {
    stat_t <- wilcox.test(as.formula(paste(var, "~ Outcome")), data)
    data0  <- data[data$Outcome == "Negative", ]
    data1  <- data[data$Outcome == "Positive", ]
    if (is.null(label)) {
        label <- var
    }
    p <- ggplot(data0, aes(x = eval(parse(text = var)))) +
        geom_histogram(
            aes(y = after_stat(count / sum(count)), fill = "Negative"),
            bins = 10, colour = "white", alpha = 0.8, boundary = 0
            ) +
        geom_histogram(data = data1,
            aes(
                x = eval(parse(text = var)),
                y = after_stat(count / sum(count)), fill = "Positive"
                ),
            bins = 10, colour = "white",
            alpha = 0.5, boundary = 0, inherit.aes = FALSE) +
            theme_bw() +
            scale_fill_manual(
                name = "",
                breaks = c("Positive", "Negative"),
                values = c("Positive" = c1, "Negative" = c2)
            ) +
            xlab(label) + ylab("Relative frequency") + ggtitle(label) +
            geom_vline(xintercept = mean(data1[[var]]), colour = c1) +
            geom_vline(xintercept = mean(data0[[var]]), colour = c2)
    p + annotate(
            "text",
            x = 0.9 * max(data1[var]),
            y = 0.9 * max(ggplot_build(p)$data[[1]]["y"]),
            label = sprintf("p-value = %.4e", stat_t$p.value),
            size = 3
        )
}
```

Let us start with the number of pregnancies.
We can see that people who have diabetes have had more
pregnancies than those who do not have diabetes.
We see that it seems to be somewhat based on the p-value alone.
We also note it exhibits a heavily right-skewed behaviour.
As such, a logarithmic transformation would make sense to get
a distribution more compatible with the normal one.
Nonetheless, a problem here arises in dealing with the null values [^2].
For that purpose we shift the variable by one unit.
As a consistency measure, we will apply this shift to all
variables we log-transform.

[^2]: Remember the domain of the logarithmic function is $(0, \infty)$.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Pregnancies")
```

The change as we see does help in obtaining a more
centered distribution with which we may better apply
the posterior analysis.

```{r, fig.dim = c(8, 4)}
data$LogPregnancies <- log(data$Pregnancies + 1)
histogram_by_groups(data, "LogPregnancies")
```

The next variable to visualize is the glucose.
People with diabetes exhibit higher glucose values.
The p-value  is very small which
indicates a highly significant difference between the two groups.
We also observe that the distribution is already
well-centered and resembles a normal distribution.
Hence, we decide not to transform the data.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Glucose")
```

We move onwards to blood pressure.
In this case, although the distributions appear
as normal, there does not seem to exist a highly significant
difference between the groups in contrast to what the p-value states,
more so compared with the glucose variable.
Nonetheless, there seem to be an slight indication
of higher blood pressure for people with diabetes.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "BloodPressure")
```

Now skin thickness is interesting, because the distributions
are visually to those of the blood pressure but the presence
of outliers is appreciable.
We will later deal with those but for now let us
mantain this variable as it is.
Note that the population with diabetes appear
to exhibit a thicker skin.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "SkinThickness")
```

Insulin, as we may have expected from the name of the property
itself, appears to be a relevant.
The median of the distributions does indeed seem to differ,
with the one for the diabetes population being
slightly higher.
It is also right-skewed, so we decide to log-transform it.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Insulin")
```

It does appear that the transformation impproves the
the symmetrization of the data, although some left-skewness
appears.
We will later see if outlier detection get to
target those values or not.

```{r, fig.dim = c(8, 4)}
data$LogInsulin <- log(data$Insulin + 1)
histogram_by_groups(data, "LogInsulin")
```

Body Mass Index (BMI) again exhibits this tendency of
leaning towards a more right-skewed distribution.
It does also follow the tendency of being
slightly higher for people with diabetes.
As such we log-transform to try and get a more normalized variable.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "BMI")
```

As we may visually judge, it is the case
that the log transformation centers the data
and provides a more normal-like distribution.
There is some presence of seemingly outlying points
to the right.

```{r, fig.dim = c(8, 4)}
data$LogBMI <- log(data$BMI + 1)
histogram_by_groups(data, "LogBMI")
```

The Diabetes Pedigree Function (DPF) is the
again a flagrant right-skewed.
It again has higher values for the positive set.
This is to be expected from the definition of this
very function as a risk indication for diabetes.
Let us try to log-transform it.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "DiabetesPedigreeFunction", "DPF")
```

The improvement is highly noticeable.
We retain thus this transformed variable.

```{r, fig.dim = c(8, 4)}
data$LogDPF <- log(data$DiabetesPedigreeFunction + 1)
histogram_by_groups(data, "LogDPF")
```

We may note that young people, as with other illnesses
have less tendency to suffer diabetes than the elders.
The age is expected to exhibit a right-skewed
distribution, as is indeed the case.
In an attempt to improve the symmetry,
we once again use logarithms to transform the variable.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Age")
```

The transformation does help although
not by much.
This is a common problem of the age variable.
We keep the transformation anyway as it does seem
to help with the symmetry for the positive group.

```{r, fig.dim = c(8, 4)}
data$LogAge <- log(data$Age + 1)
histogram_by_groups(data, "LogAge")
```

```{r}
# Some convenience
df  <- subset(data, select = -c(Pregnancies, Insulin, BMI, DiabetesPedigreeFunction, Age))
df0     <-  df[df$Outcome == "Negative", ]
df1     <-  df[df$Outcome == "Positive", ]
xnames  <- names(df)[! names(df) %in% c("Outcome")]
vec_col <- ifelse(df$Outcome == "Negative", c1, c2)
```

Now, let's take a look at some multivariate plots. We'll begin by inspecting the Parallel Coordinate Plot:

```{r, fig.dim = c(8, 4)}
par(las = 2)
parcoord(df[xnames], col = c(c1, c2))
legend("topright", legend = c("Negative", "Positive"),
       col = c(c3, c4), lty = 1, lwd = 2)
```

It seems that, overall, the positive lines are 
over negative ones. 
This is most notable on the Glucose and log
transformed BMI.
They are two of the most significant
features according to the p-value.

The Andrew's plot is the following: 

```{r, fig.dim = c(8, 4)}
andrewsplot(as.matrix(df[xnames]), df$Outcome, style = "cart")
legend("topright", legend = c("Negative", "Positive"),
       col = c("black", "red"), lty = 1, lwd = 2)
```

Again, we see that the two groups are different.
The group of people who have diabetes tend to have more volatile curves,
reaching higher and lower values along the curve.

## Multivariate characteristics and outlier identification

We are goig to use a multivariate approach
to identifying the outliers in our data.
In the process, we will need to compute the
mean and covariance.

We start then by finding the mean vector and the
covariance matrix.
In order to reduce the sensitivity to outliers in this
computation, we use a robust estimation using 
the "Fast MCD" (Minimum Covariance Determinant) estimator.
We set main parameter, `alpha`, which determines the percentage
of the data to use, at $0.85$.

<!-- TODO: refactor the prints -->

```{r}
our_corrplot <- function(cov_mat) {
    colnames(cov_mat) <- c("Log\nPregnancies",
                           "Glucose",
                           "Blood\nPressure",
                           "Skin\nThickness",
                           "LogInsulin",
                           "LogBMI",
                           "LogDPF",
                           "LogAge")
    corrplot.mixed(cov2cor(cov_mat), lower = "number", upper = "color",
        diag = "n", tl.col = "black", tl.cex = 0.65,
        lower.col = "black")
}

mcd_est <- CovMcd(df[xnames], alpha = 0.85, nsamp = "deterministic")
```

```{r, echo = FALSE, fig.size = c(8, 8)}
print("The mean vector is:")
print(mcd_est$center)
print("The covariance matrix is:")
print(round(mcd_est$cov, 4))
```

```{r, fig.size = c(8, 8)}
our_corrplot(mcd_est$cov)
```

It is interesting to seggregate by the class
of the Outcome variable.
This way, we can both, get the mean vector and covariance
matrix for each of the classes, and the outliers for both sets.

```{r, fig.size = c(8, 8)}
mcd_neg <- CovMcd(df0[xnames], alpha = 0.85, nsamp = "deterministic")
mcd_pos <- CovMcd(df1[xnames], alpha = 0.85, nsamp = "deterministic")
```

```{r, echo = FALSE, fig.size = c(8, 8)}
print("### Negative class ###")
print("The mean vector is:")
print(mcd_neg$center)
print("The covariance matrix is:")
print(round(mcd_neg$cov, 4))
```

```{r, fig.size = c(8, 8)}
our_corrplot(mcd_neg$cov)
```

```{r, echo = FALSE, fig.size = c(8, 8)}
print("### Positive class ###")
print("The mean vector is:")
print(mcd_pos$center)
print("The covariance matrix is:")
print(round(mcd_pos$cov, 4))
```

```{r, fig.size = c(8, 8)}
our_corrplot(mcd_pos$cov)
```

Let us focus first on the mean vectors.
The here observed particularities are nothing new,
as they were already present on above histograms:

- Higher number of pregnancies seems to increase
the chances on developing diabetes.

- The glucose and insulin levels of the positive
population is higher in contrast to the negative one.

Looking now at the representations for the covariance matrices,
the major changes are that the correlation between skin thickness and 
diabetes pedigree function is lower in the group who do not have diabetes
than in the one having the disease.
Moreover, the correlation between BMI [^3] and age is positive 
for the sane group while negative on the positive one.

[^3]: Remember this is the logarithm.

We now search for outliers.
The idea is to use Mahalanobis distance.
As we suppose that our data $X \sim \mathcal{N}(\mu, \Sigma)$,
we have $D_M(x,\mu)^2 \sim \chi_p^2$, with $D_M$ the
Mahalanobis distance.
Hence we may set our outlier criteria as
\begin{equation}
    D_M(x, \mu)^2 > \chi_{p, 0.95^{1/n}}^2 \;,
\end{equation}
the $0.95^\text{th}$ quantile of the $\chi_p^2$ distribution.
We then drop these outliers.

```{r}
p   <- length(xnames)
n0  <- nrow(df0)
n1  <- nrow(df1)
df0_clean <- df0[mcd_neg$mah < qchisq(0.95^(1 / n0), p), ]
df1_clean <- df1[mcd_pos$mah < qchisq(0.95^(1 / n1), p), ]
df_clean  <- rbind(df0_clean, df1_clean)
# Re shuffle the data
df_clean  <- df_clean[sample(seq_len(nrow(df_clean))), ]
vec_col   <- ifelse(df_clean$Outcome == "Negative", c1, c2)
```

```{r, echo = FALSE}
print(paste("The negative set contained", n0 - nrow(df0_clean), "outliers."))
print(paste("The positive set contained", n1 - nrow(df1_clean), "outliers."))
```

As a final summary of this section, we
perform a plot in which the histograms of different populations
are observed, as well as scatterplots of pairs of variables
and correlations.

```{r, fig.dim=c(16, 16)}
ggpairs(df_clean, aes(color = Outcome), legend = 1,
        columns = xnames,
        diag = list(continuous = "barDiag")
        ) +
    theme(legend.position = "bottom") +
    scale_fill_manual(values = c(c1, c2)) +
    scale_color_manual(values = c(c1, c2)) + labs(fill = "Outcome")
```

# Multivariate analysis and dimensionality reduction

Our data has few variables, which makes dimensionality
reduction not crucial for the predictive process.
However, we may still extract important information and
insight from these techniques.

## Principal Component Analysis

The first of the techniques we will employ is
Principal Component Analysis (PCA).
A main assuption when using this technique is the
normality of the variables.
We already know that they do not follow a normal distribution,
but we may approximately suppose they do.
Before the actual computation is made, we
must scale our data.
In order to do so, we perform a univariate
scaling because our different variables have
different ranges and units.

```{r}
df_sc <- as.data.frame(scale(df_clean[, xnames]))
df_sc$Outcome <- df_clean$Outcome
```

```{r, fig.dim=c(8, 4)}
df_pcs <- prcomp(df_sc[, xnames])

df_two <- as.data.frame(df_pcs$x[, 1:2])
df_two$group <- df_clean$Outcome
colnames(df_two) <- c("x", "y", "group")

ggplot(df_two, aes(x = x, y = y, col = group)) +
    geom_point() +
    xlab("First PC") + ylab("Second PC") + ggtitle("First two PCs") +
    scale_colour_manual(values = c(c1, c2)) +
    theme_bw() + labs(col = "Group")
```

Here we see something important.
The first principal component (PC) does a reasonably
good job at seggregating both groups.
However, it performs better in clustering the negative population
than the positive one.
On the other hand, the second PC appears to be agnostic
of the objective value.
This implies that variables whose variance is mainly explained
by the first PC are going to play a more crucial role
in distinguishing both groups.

With respect to the last point, it is of usefulness
to examine how the different variables contribute to
these two principal components.
We will do so in two steps.
First, we look infividually at the two PCs with a loading plot.
Then, we will draw a biplot.

Starting with the first PC, we note that all the variables
contribute positively and around the same amount save
from the LogDPF which lags behind.

```{r, fig.size = c(8, 4)}
df_temp <- data.frame(x = seq_along(xnames), y = df_pcs$rotation[, 1])

ggplot(df_temp, aes(x = x, y = y)) +
    geom_point() +
    geom_label(label = xnames, label.size = 0.3) +
    xlab("Variables") + ylab("Loadings") + ggtitle("First PC") +
    theme_bw() + xlim(c(-2, 10)) + ylim(-0.01, 0.5) +
    scale_x_continuous(breaks = c((-2):10)) +
    geom_hline(yintercept = 0)
```

The loadings of the second PC tell another story.
Here we see that LogPregnancies and LogAge have a negative loading
while SkinThickness and LogBMI have a positive one.
The rest on the other hand seem to not have a significant contribution.
Nonetheless, as before stated, this PCA seems unable to effectively
seggregate the data into the two outcome groups.

```{r, fig.size = c(8, 4)}
df_temp <- data.frame(x = seq_along(xnames), y = df_pcs$rotation[, 2])

ggplot(df_temp, aes(x = x, y = y)) +
    geom_point() +
    geom_label(label = xnames, label.size = 0.3) +
    xlab("Variables") + ylab("Loadings") + ggtitle("Second PC") +
    theme_bw() + xlim(c(-2, 10)) + ylim(-0.5, 0.8) +
    scale_x_continuous(breaks = c((-2):10)) +
    geom_hline(yintercept = 0)
```

A nice and visual summary is obtained with a biplot.
Here we may extract two main conclusions.
The first is that the variables are divided into three
clear groups somewhat uncorrelated with high correlations inside them.
A special note is that LogDPF seems to not follow the general tendency,
something that was already hinted in the loadings plots.
We will later try to see the origin of this discrepancy.
The second conclusion is that given the first PCA is able to
properly seggregate the outcomes and that, save for LogDPF,
all variables contribute on a similar ammount to the first PCA,
it appear that all variables will play an important role and we
cannot drop them from the posterior analysis [^4].

[^4]: At least using this approach. Some later methods will carry
different dimensionality reduction techniques which may or may not
discard some variables.

```{r, fig.size = c(8, 8)}
biplot(df_pcs, cex = c(0.5, 0.8))
```

Looking into the variace explained by each PC, we see
that the first two, on which we have based
our previous analysis, retain around two thirds
of the total variance.
This diagram is also useful to determine which number of
PCAs should we keep if we were to perform actual
dimensionality reduction.
For this case, there is not a strongly clear knee point
on which to claim optimality.
Based on the graph, we would place it somewhere
between 3 and 5 PCAs.
Let us for the following consider 4, equivalent
to setting a threshold at $10\%$ of explained variance.

```{r, fig.size = c(8, 4)}
fviz_eig(df_pcs, ncp = 17, addlabels = TRUE, barfill = c1, barcolor = c2)
```

Looking at the scatterplots
we once again observe that the best seggregating
component is the first, clearly outperforming
the rest of the PCAs by a wide margin, which prove
themselves kind of agnostic to the objective variable.
This agnosticity even seems to increment as the variance
explained decreases, i.e. higher PCs.

```{r, fig.size = c(16, 16)}
pairs(df_pcs$x[, 1:4], col = vec_col)
```

Looking at the correlations we kind of observe
the same we have been observing since the loadings,
big positive correlations from almost all the variables
with the first PCA.
However, we can now see to which PC does the LogDPF contribute.
We see it accounts for the vast majority of the fourth PC.
Regarding the third PC, we see noticeable negative correlations
with Glucose and LogInsulin, but nothing significant appart from that.

```{r, fig.size = c(8, 8)}
corrplot(cor(df_clean[, xnames], df_pcs$x[, 1:4]), is.corr = TRUE)
```

## Independent Component Analysis

Let us now make use of the independent component analysis (ICA) to extract
more information.
If we plot all of them, we note that some do not follow a normal distribution
but are instead right-skewed.
This happens for the fifth to eighth independent components (ICs).
The second one does also diverge from normality but in a left-skewed manner.

```{r, results = FALSE, fig.size = c(12, 12)}
df_trans_ica <- icafast(df_clean[, xnames], nc = length(xnames), alg = "par")
z <- df_trans_ica$S
colnames(z) <- sprintf("IC-%d", seq(8))
n <- nrow(df)
z <- z * sqrt((n - 1) / n)

par(mfrow = c(3, 3))
sapply(colnames(z),
    function(cname) {
        hist(as.data.frame(z)[[cname]],
             main = cname, col = c2, xlab = "")
    })
```

We now look at the negative entropy of them and note that two of
the ICs, the third and the eighth have a much higher negative entropy
than the rest.
As such, we now want to see if they are good candidates for seggregating
our data into the two outcome groups.

```{r, fig.size = c(8, 4)}
neg_entropy <- function(z) {
    mean(z^3)^2 / 12 + mean(z^4)^2 / 48
}

z_neg_entropy <- apply(z, 2, neg_entropy)
ic_sort  <- sort(z_neg_entropy, decreasing = TRUE, index.return = TRUE)$ix
z_ic_imp <- z[, ic_sort]

par(mfrow = c(1, 1))
plot(z_neg_entropy[ic_sort], type = "b", col = c1, pch = 19,
     ylab = "Neg-entropy", main = "Neg-entropies",
     lwd = 3, xaxt = "n"
     )
axis(side = 1, at = 1:8, labels = ic_sort, las = 1)
```

Unfortunately, they are not able to seggregate the groups,
but we may use this plot to identify certain may-be outliers
which lie in the negative dispersion of the eighth IC.
An appropiate condition to select them could be
\begin{equation}
    \text{IC}-8 < -2.5 \;.
\end{equation}

```{r, fig.size = c(8, 4)}
df_temp <- data.frame(x = z_ic_imp[, 1], y = z_ic_imp[, 2], group = df_clean$Outcome)
ggplot(df_temp, aes(x = x, y = y, col = group)) + geom_point() +
    xlab("Third IC") + ylab("Eighth IC") +
    ggtitle("ICs with more neg-entropy") +
    scale_colour_manual(values = c(c1, c2)) +
    theme_bw() + labs(col = "Group")
```

Following the same workflow we used for PCA, we now
represent an scatterplot of all of the ICs to look
for useful ICs which may be able to help in a
clusterization of our data based on having or not diabetes.

For our case, there was not a pair of ICs which was able to
separate the two groups in a significant way.
As such, we belive we will not get much more information
using this approach.
Nonetheless, by visualizing the covarariances of the
ICs with respect to the original variables, we note that
each of the ICs is highly correlated to at most two attributes.
A conclusion we may extract is that there are no simple combination
of of attributes which may seggregate impactfully the outcome [^5].

[^5]: This is slightly redundant information, as from the attribute
scatter plot we did already know that there was not any pairplot capable
of splitting clearly the groups.

```{r, fig.size = c(16, 16)}
pairs(z_ic_imp, col = vec_col)
```

```{r, fig.size = c(8, 8)}
corrplot(cor(df_clean[, xnames], z_ic_imp), is.corr = TRUE)
```

<!-- 
#```{r}
colnames(df_pcs$x) <- paste0("PC", 1:8)
corrplot(cor(df_pcs$x, z_ic_imp), lower.col = "black")
```
-->

## Factor Analysis

The last of the techniques we are going to employ is the
factor analysis.
The motivation lies in the following plot, where we observe
that some groups of variables are correlated,
suggesting a factor structure.
Note that we expect these groups and the findings here to
be similar to those observed in the previously drawed biplot.
However, if we may
reach to the same conclusions from different approaches it will strengthen
our conclusions on this regard.

```{r, fig.size = c(8, 8)}
corrplot(cor(df_clean[, xnames]), order = "hclust")
```

Some mathematical tinkering needs to be done as to extract
the loadings of the factors.
We refer to the lecture notes for the details on the operations
and focus on the thoughts we may extract.

```{r}
m_0 <- df_pcs$rotation[, 1:3] %*% diag(df_pcs$sdev[1:3])
s_y <- cov(df_sc[, xnames])
mm  <- s_y - diag(diag(s_y - m_0 %*% t(m_0)))
mm_eig <- eigen(mm)
m_1 <- mm_eig$vectors[, 1:3] %*% sqrt(diag(mm_eig$values[1:3]))

m_vrm <- loadings(varimax(m_1))[seq_along(xnames), 1:3]
```

On the following we attach the loadings for the first
three factors.
Let us look at them and the insight they might provide.

The two main attributes which contribute to the first
factor are LogBMI and the skin thickness.
These do correspond to what we observed in the biplot.
Note that we are not medical experts, but we may suppose
that this relation comes from the fact that skin thickness
is usually related with high presence of fat, which increases
the value of the Body Mass Index.

The second factor presents a more challenging scenario
which we cannot immediately identify with a previous cluster
in the biplot.
Unfortunately we lack a medical expertise which could
identify this kind of plot.
What we can clearly say is that the second factor is dominated
by the blood pressure.

As with the second, the third factor does not show a clear
dominance of any of the attributes over the rest.
It does provide an interesting observation and is that the
two variables which lie futher away in this graph are the age
and the number of pregnancies, which appeared clustered on the PCA.

```{r, fig.size = c(8, 4)}
plot(seq_along(xnames), m_vrm[, 1], pch = 19,
    col = c1, xlim = c(-1, 9), ylim = c(-1.1, 0.1),
    xlab = "", ylab = "Loadings",
    main = "Loadings for the first factor")
abline(h = 0)
text(seq_along(xnames), m_vrm[, 1], labels = xnames, pos = 1, col = c2, cex = 0.75)
```

```{r, fig.size = c(8, 4)}
plot(seq_along(xnames), m_vrm[, 2], pch = 19,
    col = c1, xlim = c(-1, 9), ylim = c(-0.05, 0.4),
    xlab = "", ylab = "Loadings",
    main = "Loadings for the second factor")
abline(h = 0)
text(seq_along(xnames), m_vrm[, 2], labels = xnames, pos = 1, col = c2, cex = 0.75)
```

```{r, fig.size = c(8, 4)}
plot(seq_along(xnames), m_vrm[, 3], pch = 19,
    col = c1, xlim = c(-1, 9), ylim = c(-0.1, 0.3),
    xlab = "", ylab = "Loadings",
    main = "Loadings for the third factor")
abline(h = 0)
text(seq_along(xnames), m_vrm[, 3], labels = xnames, pos = 1, col = c2, cex = 0.75)
```

## Conclusions

This section is comprised of a high number of graphs which
may lead us away from the insight they bring.
As such, we here present the conclusions that we were
able to extract from all the analysis:

- The objective variable does not seem to exhibit
a clear dependence from one or few of the attributes,
but of all of them.
It is worth to mention that the first PC does seem
able to approximately distinguish clearly negative
individuals, but fails in the more complicated instances.

- The variables are correlated between them forming
groups.
This is, in general, due to medical reasons, which
unfortunately we are not able to explain in detail.
Nonetheless, none of the variables appeared redundant.

- Some outliers may be considered to still exist on
the data.
However, given they do not seem crucial for the analysis,
we decide to keep them.

<!-- Definitive datasets:
- df_clean for the cleaned data (already shuffled)
- df_sc for the cleaned and scaled data (already shuffled)

To access the attributes: df_whatever[, xnames]

To access the objective function: df_whatever$Outcome -->


<!-- definitive data: df_clean -->
<!-- definitive attributes: df_clean[, xnames] -->
<!-- definitive objective variable: df_clean$Outcome -->


```{r, echo = F}
y2 <- dplyr::recode(df_clean$Outcome, Negative = 0, Positive = 1)
X2 <- as.matrix(df_sc[, xnames])
```

# Unsupervised Learning

```{r, echo = F}
cluster_means <- function(x, data = df_clean[,xnames], first = 1){
  if(first == 0){
    x <- x + 1
  }
  ls <- list()
  groups_ <- sort(unique(x))
  for (i in groups_){
    ls[[i]] <- data[x == i,]
  }
  z <- sapply(ls, function(x) sapply(x, mean, na.rm = T))
  z <- as.data.frame(z)
  colnames(z) <- paste("Cluster", 1:length(groups_), sep = " ")
  z2 <- z
  return(z2)
}

cluster_classification <- function(x, y = y2, first = 1){
  x <- x - first
  prev <- table(x, y)
  if(prev[1,1] + prev[2,2] > prev[1,2] + prev[2,1]){
    f <- prev
  } else{
    f <- table(1-x, y)
  }
  dimnames(f) <- list("Pred" = c("Negative", "Positive"), "Actual" = c("Negative", "Positive"))
  f2 <- f
  return(f)
}
```

## Partitional clustering

### Number of clusters

We'll firstly decide how many clusters we want to create. We'll use the silhoutte method and the gap statistic. The silhoutte method:

```{r}
fviz_nbclust(X2, kmeans, method = "silhouette", k.max = 10)
```
As we can see, this indicates that the optimum number of clusters is 2.

The gap statistic:

```{r, results = "hide"}
gap_stat <- clusGap(X2, FUN = kmeans, K.max = 10, B = 100)
```
```{r}
fviz_gap_stat(gap_stat, linecolor = c1, maxSE = 
                list(method = "firstSEmax", SE.factor = 1))
```
This suggests that the optimum number for k is 3. However, we can see that all values of k get very similar gap statistics, so we will proceed with k = 2, as it also is the number of classes that our problem has. 

### K-Means

The results of k-means clustering with 2 clusters are the following: 

```{r}
kmeans_X <- kmeans(X2, centers = 2, iter.max = 1000, nstart = 100)

kmeans_col <- c(c1, c2)[kmeans_X$cluster]
plot(df_two[,1:2], col = kmeans_col, main = "First two PCs", 
     xlab = "First PC", ylab = "Second PC", pch  = 16)
```

Let's see the average in each variable for the clusters:

```{r}
kmeans_means <- cluster_means(kmeans_X$cluster)
kable(kmeans_means) %>% kable_styling(latex_options = "HOLD_position")
```

We can also see the confusion matrix with our target variable, although we must remember that this is not the objective of unsupervised learning.

```{r}
table_kmeans <- cluster_classification(kmeans_X$cluster, first = 1)
kable(table_kmeans) %>% kable_styling(latex_options = "HOLD_position")
```

As we can see, the clusters are not exactly coincident with the target variable, but it does a good job (`r round((table_kmeans[1,1]+table_kmeans[2,2])/757, 2)`% accuracy).

### K-Medoids

As all of our attributes are numerical and we don't have any reason not to, we will use Euclidean distance.

```{r}
pam_X <- pam(X2, k = 2, metric = "euclidean", stand = FALSE)
pam_col <- c(c1, c2)[pam_X$cluster]

plot(df_two[,1:2], col = pam_col, main = "First two PCs", 
     xlab = "First PC", ylab = "Second PC", pch = 19)
```

We obtain a very similar result than with K-Means, although they separation between groups in the first PCs is diagonal, whereas in the K-Means it was vertical. We can take a look into the averages (we do not see the medoids because they were computed with the scaled data):

```{r}
pam_means <- cluster_means(pam_X$cluster, first = 1)
kable(pam_means) %>% kable_styling(latex_options = "HOLD_position")
```
Again, the confusion matrix is the following:

```{r}
kable(cluster_classification(pam_X$cluster))
```
Very similar results than with K-Means.

### CLARA

```{r}
clara_X <- clara(X2, k = 2, metric = "euclidean", stand=FALSE)
clara_col <- c(c1, c2)[clara_X$cluster]

plot(df_two[,1:2], col = clara_col, main = "First two PCs", 
     xlab = "First PC", ylab = "Second PC", pch = 19)
```
We can observe that the results are not very similar than with K-Means and K-Medoids. There are `r table(clara_X$cluster)[1]` instances in one group and `r table(clara_X$cluster)[2]` in the other. 

```{r}
clara_means <- cluster_means(clara_X$cluster, first = 1)
kable(clara_means) %>% kable_styling(latex_options = "HOLD_position")
```

```{r}
cluster_classification(clara_X$cluster)
```
## Hierarchical clustering

### Single linkage

```{r}
euc_dist_X <- daisy(X2, metric = "euclidean")
single_X <- hclust(euc_dist_X, method = "single")
plot(single_X, main = "Single linkage", cex = 0.8)
rect.hclust(single_X, k = 2, border = c1)
```

This does not seem to yield good results. The clusters created are very, very, very small. 

```{r}
cl_single_X <- cutree(single_X, 2)
table(cl_single_X, y2)
```
Only two one instance belongs to the second cluster.

```{r}
sil_single_X <- silhouette(cl_single_X, euc_dist_X)
factoextra::fviz_silhouette(sil_single_X)
```

### Complete linkage

```{r}
complete_X <- hclust(euc_dist_X, method = "complete")
plot(complete_X, main = "Complete linkage", cex = 0.8)
rect.hclust(complete_X, k = 2, border = c1)
```

```{r}
cl_complete_X <- cutree(complete_X, 2)
table(cl_complete_X, y2)
```
Now, we can see that the sizes of the clusters are much more similar, although one is twice as big as the other. 

```{r}
sil_complete_X <- silhouette(cl_complete_X, euc_dist_X)
factoextra::fviz_silhouette(sil_complete_X)
```

### Average linkage

```{r}
average_X <- hclust(euc_dist_X, method = "average")
plot(average_X, main = "Average linkage", cex = 0.8)
rect.hclust(average_X, k = 2, border = c1)
```

As with the single linkage, the clusters created by this method are very small and does not seem adequate in this case. 

```{r}
cl_average_X <- cutree(average_X, 2)
table(cl_average_X, y2)
```

```{r}
sil_average_X <- silhouette(cl_average_X, euc_dist_X)
factoextra::fviz_silhouette(sil_average_X)
```

### Ward linkage

```{r}
ward_X <- hclust(euc_dist_X, method = "ward.D")
plot(ward_X, main = "Ward linkage", cex = 0.8)
rect.hclust(ward_X, k = 2, border = c1)
```

```{r}
cl_ward_X <- cutree(ward_X, 2)
table(cl_ward_X, y2)
```

The sizes of the clusters are almost identical, and it actually resembles the groups of the target variable (`r round((table(cl_ward_X, y2)[1,1]+table(cl_ward_X, y2)[2,2]) / 757, 2)`% accuracy). 

```{r}
sil_ward_X <- silhouette(cl_ward_X, euc_dist_X)
factoextra::fviz_silhouette(sil_ward_X)
```

## DBSCAN

We fix MP to 5, and we'll see the k-distance graph to see what would be a good value for $\epsilon$.

```{r}
minPts <- 5
kNNdistplot(X2, k = minPts - 1)
abline(h = 1.8, col = c4)
```

```{r}
db_fit <- dbscan(X2, eps = 1.8, minPts = 5)
colors_db <- c(c1, c2, c3)[db_fit$cluster + 1]
plot(df_two[,1:2], pch = 19, col = colors_db, xlab = "First PC",
     ylab = "Second PC")
```

```{r}
table(db_fit$cluster)
```

The clusters have very different sizes, and this is an indicator of a unique region with high data density. The averages are:

```{r, echo = F}
db_means <- cluster_means(db_fit$cluster, first = 0)
kable(db_means) %>% kable_styling(latex_options = "HOLD_position")
```

We can also see the confusion matrix with our target variable, although we must remember that this is not the objective of unsupervised learning.

```{r}
kable(cluster_classification(db_fit$cluster, first = 0)) %>% kable_styling(latex_options = "HOLD_position")
```

## Model-based clustering

```{r}
BIC_X <- mclustBIC(X2, G = 1:5)
plot(BIC_X)
summary(BIC_X)
```

The suggested number of clusters is 4. However, we can see that there is not much difference between choosing 2, 3, 4 and 5. The best models are EEE (ellipsoidal, equal volume, shape and orientation) with 4 clusters, and VEE (ellipsoidal, equal shape and orientation) with 4 and 5 clusters. 

```{r}
Mclust_X <- Mclust(X2, x = BIC_X, verbose = F)
summary(Mclust_X)
```

```{r}
mb_means <- cluster_means(Mclust_X$classification, first = 1)
kable(mb_means) %>% kable_styling(latex_options = "HOLD_position")
```

## Conclusions

We have tried different methods for clustering our data. With the partitional methods, we have chosen two clusters and, while K-Means and K-Medoids obtain similar results, CLARA differs from them and creates clusters of unequal size. 

With hierarchical clustering, we have observe that they create clusters of very unequal sizes except for the Ward and the complete methods. The Ward which would be our choice if we were to select one of them. 

Model-based clustering suggested 4 as the optimum number of clusters, another evidence that suggests that the dependent variable does not condition all of the variables. 

DBSCAN is not a good choice either, because all of the data seems to be grouped together and the method creates a cluster with very few instances, and another with almost all of the instances.

Neither of them cluster the data into the groups that the dependent variable creates, but that makes sense as it is not its objective and our data does not appear to be separable by that. 


# Supervised Classification

We split the data into the predictors ($X$) and the variable we want to predict ($Y$):
```{r}
X <- df_clean[,xnames]
Y <- df_clean$Outcome
```

```{r}
n <- nrow(X)
p <- ncol(X)
c(n,p)
```

```{r}
n_no <- sum(Y=="Negative")
n_yes <- sum(Y=="Positive")
c(n_no,n_yes)
```

```{r}
pr_no <- n_no/n
pr_yes <- n_yes/n
c(pr_no,pr_yes)
```
To create the training and test partitions we will make a 70/30 partition, that is, 70% of the individuals will go to the training partition and the remaining 30% to the test partition.

In order to do that, first we are going to compute the number of individuals of each partition:

```{r}
n_train <- floor(.7*n) ## 70/30 partition for train and test
n_test <- n - n_train
c(n_train,n_test)
```

With that computed, we can generate the index of the individuals that are going to belong to the training partition:
```{r}
i_train <- sort(sample(1:n,n_train))
```
The individuals that are going to belong to the testing partition will be the ones that do not belong to the training partition, thus we can generate the training and testing partitions by:

```{r}
X_train <- X[i_train,]
X_test <- X[-i_train,]
Y_train <- Y[i_train]
Y_test <- Y[-i_train]
```

It may be interesting to check the proportion of individuals of each class in both the train and the test partitions. 

```{r}
np_train <- sum(Y_train=="Negative")/n_train; np_train
pp_train <- sum(Y_train=="Positive")/n_train; pp_train
```

```{r}
np_test <- sum(Y_test=="Negative")/n_test; np_test
pp_test <- sum(Y_test=="Positive")/n_test; pp_test
```


As the problem is unbalanced, we want to define some key concepts before starting with the methods.

The confusion matrix is defined as:

```{r, echo=FALSE}
table <- data.frame("Classified_as_negative" = c("True negative (TN)", "False negative (FN)"),
                    "Classified_as_positive" = c("False positive (FP)", "True positive (TP)"))
rownames(table) <- c("Instances_actually_negative", "Instances_actually_positive")

kable(table) %>% kable_styling(latex_options = "striped") %>% kable_styling(latex_options = "HOLD_position")
```
With this in mind, we can also define:

- $TPR=TP/(TP+FN)$ (True Positive Rate i.e. Sensitivity)
- $FNR=FN/(TP+FN)$ (False Negative Rate)
- $FPR=FP/(FP+TN)$ (False Positive Rate)
- $TNR=TN/(FP+TN)$ (True Negative Rate i.e Specificity)
- $\text{Accuracy}=(TP+TN)/(TP+TN+FP+FN)$
- $\text{TER} = (FP+FN)/(TP+TN+FP+FN) = 1 - \text{Accuracy}$ (Text Error Rate)
- $BAC=(TPR+TNR)/2$ (Balanced Accuracy)

## K-Nearest Neighbors (KNN)

The first supervised classification method that we are going to test is K-Nearest Neighbors.

First, we are going to find the optimum value for $K$ value using cross-validation.

```{r}
LER <- rep(NA,20)
for (i in 3 : 20){
  knn_output <- knn.cv(X_train,Y_train,k=i)
  LER[i] <- 1 - mean(knn_output==Y_train)
}
LER
plot(1:20,LER,pch=20,col=c1,type="b",
     xlab="k",ylab="LER",main="LER for the diabetes dataset")
```

It seems that the optimal $K$ is

```{r}
k <- which.min(LER)
k
```

Now, with the K selected, we can now compute the predictions for the testing partition.

```{r}
knn_Y_test <- knn(X_train,X_test,Y_train,k=k,prob=T)
```

With the predictions computed, we can show the confusion matrix:
```{r}
cm_knn <- confusionMatrix(table(Y_test,knn_Y_test), positive = "Positive")

cm_knn$table %>% kable() %>% kable_styling(latex_options = "striped") %>% kable_styling(latex_options = "HOLD_position")
```

Looking at some of the metrics defined earlier, we can see that the results are not great.

```{r}
cm_knn$overall["Accuracy"]
cm_knn$byClass["Sensitivity"]
cm_knn$byClass["Specificity"]
```


We compute the Test Error Rate by:

```{r}
knn_TER <- mean(Y_test!=knn_Y_test)
knn_TER
```

Given a confusion matrix, we can also compute the BAC. First, we define the following function:
```{r}
bac <- function(cm){
  return((cm[1,1]/(cm[1,1] + cm[1,2]) + (cm[2,2]/(cm[2,1] + cm[2,2])))/2)
}
```

And we compute the BAC:
```{r}
bac(cm_knn$table)
```
```{r, echo=F}
predictors_tables <- list()
predictors_tables[[1]] <- cm_knn
```


It is noticeably lower than the accuracy.


Finally, we are going to plot the probability of being positive on diabetes for each instance.
The ones in green are correctly classified, while those in red have been classified incorrectly.

```{r}
prob_knn_Y_test <- ifelse(knn_Y_test == "Positive", attributes(knn_Y_test)$prob, 1 - attributes(knn_Y_test)$prob)
prob <- 2*ifelse(knn_Y_test == "-1", prob_knn_Y_test, 1-prob_knn_Y_test) - 1
colors_errors <- c(c4,c3)[1*(Y_test==knn_Y_test)+1]
plot(1:n_test,prob_knn_Y_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Diabetes probabilities",main="Diabetes probabilities")
```


## Logistic Regression

Next, we are going to test is logistic regression. We start by training the model and checking the summary:
```{r}
lr_train <- multinom(Y_train~.,data=X_train)
summary(lr_train)
```

In order to see which are the most significant coefficients, we can use the t-test:

```{r}
t_test_lr_train <- summary(lr_train)$coefficients/summary(lr_train)$standard.errors
sort(abs(t_test_lr_train),decreasing=TRUE)
```
We can see that the most important variables are *Glucose*, *LogBMI*, *LogDPF* and *LogAge*.

Next, we make the predictions and check the number of instances classified in group:
```{r}
lr_test <- predict(lr_train,newdata=as.data.frame(X_test))
summary(lr_test) %>% kable() %>% kable_styling(latex_options = "striped") %>% kable_styling(latex_options = "HOLD_position")
```
 
We compute the confusion matrix:
```{r}
cm_lg_default <- confusionMatrix(table(Y_test,lr_test))
cm_lg_default$table %>% kable() %>% kable_styling(latex_options = "striped") %>% kable_styling(latex_options = "HOLD_position")
```
It can be observed that the model classifies well those individuals who do not have diabetes, but has a hard time classifying those individuals who have diabetes. This is because the dataset is not balanced.

Now, we compute both the TER and the accuracy:

```{r}
lr_TER <- mean(Y_test!=lr_test)
lr_TER
lr_ACC <- 1 - lr_TER
lr_ACC
```
The value obtained is not bad, but we must keep in mind that the classifier is doing really bad in classifying diabetic individuals. This can be seen more clearly if we calculate the Balanced Accuracy (BAC).


We compute the BAC by using it:
```{r}
bac(cm_lg_default$table)
```

It is observed that the value obtained is noticeably worse than the accuracy, as it balances sensitivity and specificity. We can also check those:

```{r}
cm_lg_default$byClass["Sensitivity"]
cm_lg_default$byClass["Specificity"]
```
The results are not good.

```{r}

prob_lr_test <- 1 - predict(lr_train,newdata=X_test,type ="probs")
head(prob_lr_test)

colors_errors <- c(c4,c3)[1*(Y_test==lr_test)+1]
plot(1:n_test,prob_lr_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of Non-Diabetes",
     main="Probabilities of Diabetes")
abline(h=0.5)
```

We are not happy with the performance of the model so we are going to try to improve it.

In order to try to improve it, we are going to first do stepwise model selection. 
The information criterion that we are going to use in order to evaluate the measure the performance of the model is the *Bayesian Information Criterion* (BIC), and it is defined as

$$
BIC = -2l(model)+npar(model)\cdot\log(n)
$$
and it aims to balance the model complexity and fitness. We do the stepwide model selection:

```{r}
### BIC
final_df <- as.data.frame(cbind(X_train, Y_train))
final_df$Y_train <- ifelse(final_df$Y_train=="Negative", 0, 1)
mod_zero  <- glm(Y_train ~ 1, family = binomial, data = final_df)
mod_all   <- glm(Y_train ~ ., family = binomial, data = final_df)
model_glm <- MASS::stepAIC(mod_zero, scope = list(lower = mod_zero, upper = mod_all), direction = "both", trace = 0, k = log(nrow(final_df)))
```

```{r}
model_glm$coefficients
```

We can see that the variables selected by the model are the same as the four more significant variables selected by the t-test. 

By predicting with the new model we obtain very similar results as the previous ones:

```{r}
X_test_2 <- X_test[, names(model_glm$coefficients)[-1]]

pred <- predict(model_glm, X_test_2, type="response") > 0.5
pred <- c("Negative", "Positive")[(1*pred)+1]
cm <- table(Y_test, pred)
cm %>% kable() %>% kable_styling(latex_options = "striped") %>% kable_styling(latex_options = "HOLD_position")
```

And almost the same BAC:

```{r}
BAC <- bac(cm)
BAC
```

As the logistic regression is a scoring classifier (i.e. a classifier that predicts a real value representing the probability that the instance belongs to a certain class, in our case, to be positive in diabetes) we can choose the threshold we can define the threshold at which we decide that the instance is considered to be positive for diabetes. Until now, we have been using $0.5$ as the threshold.

Note that this is one of the many options available to handle imbalanced problems. Other approaches may be oversampling or undersampling, but we have chosen to use thresholding because we are using scoring classifiers.

We define a function that takes the model, the data and a given threshold and returns the prediction given that threshold.

```{r}
get_logistic_pred = function(model, data, threshold = 0.5) {
  probs = predict(model,newdata=data,type="response")
  ifelse(probs > threshold, "Positive", "Negative")
}
```

Thus, we can compute the the new predictions for different cuts ($0.1$, $0.5$ and $0.9$) by:
```{r}
test_pred_10_lg = get_logistic_pred(model_glm, data = X_test_2, threshold = 0.1)
test_pred_50_lg = get_logistic_pred(model_glm, data = X_test_2, threshold = 0.5)
test_pred_90_lg = get_logistic_pred(model_glm, data = X_test_2, threshold = 0.9)
```
For these thresholds, we now can compute they accuracy, sensitivity and specificity:
```{r}
test_tab_10_lg = table(predicted = test_pred_10_lg, actual = Y_test)
test_tab_50_lg = table(predicted = test_pred_50_lg, actual = Y_test)
test_tab_90_lg = table(predicted = test_pred_90_lg, actual = Y_test)

test_con_mat_10_lg = confusionMatrix(test_tab_10_lg, positive = "Positive")
test_con_mat_50_lg = confusionMatrix(test_tab_50_lg, positive = "Positive")
test_con_mat_90_lg = confusionMatrix(test_tab_90_lg, positive = "Positive")

metrics = rbind(
  c(test_con_mat_10_lg$overall["Accuracy"], 
    test_con_mat_10_lg$byClass["Sensitivity"], 
    test_con_mat_10_lg$byClass["Specificity"]),
  
  c(test_con_mat_50_lg$overall["Accuracy"], 
    test_con_mat_50_lg$byClass["Sensitivity"], 
    test_con_mat_50_lg$byClass["Specificity"]),
  
  c(test_con_mat_90_lg$overall["Accuracy"], 
    test_con_mat_90_lg$byClass["Sensitivity"], 
    test_con_mat_90_lg$byClass["Specificity"])
  
)
rownames(metrics) = c("c = 0.10", "c = 0.50", "c = 0.90")
colnames(metrics) = c("Accuracy", "Sensitivity", "Specificity")

metrics %>% kable() %>% kable_styling(latex_options = "striped") %>% kable_styling(latex_options = "HOLD_position")
```
As can be seen, with the threshold=$0.1$ we obtain a high sensitivity but a low specificity and, for the threshold=$0.9$ a low sensitivity but a low specificity. We can plot a ROC curve to search for the best threshold.
```{r}
test_prob = predict(model_glm, newdata = X_test_2, type = "response")

ROC_lr = plot.roc(Y_test, test_prob,
         main="Confidence interval of a threshold", percent=TRUE,
         ci=TRUE, of="thresholds",
         thresholds="best", 
         print.thres="best",
         print.auc=TRUE) 
```
It is around `r round(as.numeric(rownames(ROC_lr$ci$specificity)),2)`.
```{r}
ROC_lr
```

If we compute the predictions with the new threshold:

```{r}
pred <- predict(model_glm, X_test_2, type = "response") > 
  as.numeric(rownames(ROC_lr$ci$specificity))
pred <- pred * 1
pred <- as.factor(c("Negative", "Positive")[(1*pred)+1])
cm_lr <- `confusionMatrix`(table(Y_test, pred), positive="Positive")
cm_lr$table %>% kable() %>% kable_styling(latex_options = "striped") %>% kable_styling(latex_options = "HOLD_position")
```
```{r, echo=FALSE}
predictors_tables[[2]] <- cm_lr
```


We can see a much better confusion matrix, and, if we compute de BAC:

```{r}
BAC <- bac(cm_lr$table)
BAC
```
It has improved quite a bit.

Finally, we plot the probability of diabetes for each instance, as well as the threshold.
```{r}
colors_errors <- c(c4,c3)[1*(Y_test==pred)+1]
plot(1:n_test,prob_lr_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of Diabetes",
     main="Probabilities of Diabetes")
abline(h=as.numeric(rownames(ROC_lr$ci$specificity)))
```


## Methods based on the Bayes Theorem

We are going to try three methods based on the Bayes Theorem:

- Linear discriminant analysis
- Quadratic discriminant analysis
- Naive Bayes

These methods estimate the prior probabilities as the proportion of available observations belonging to each class and they make predictions selecting the class for which the posterior probability becomes maximum.

As they take into account the prior probabilities, these methods are very convenient for unbalanced problems (as the problem we are facing), so minimal tuning may be needed.

### Linear discriminant analysis (LDA)

First, we train the model by:

```{r}
lda_train <- lda(Y_train~.,data=X_train)
```
As it is a bayesian method, it consider prior probabilities so it is taking into account that the dataset is balanced. The prior probabilities are:

```{r}
lda_train$prior
```
that are the same as the proportion of people with and without diabetes in our dataset:
```{r}
c(pr_no, pr_yes)
```

With the model fitted, we can make the predictions as:

```{r}
lda_test <- predict(lda_train,newdata=X_test)
```

And save the predicted classes as:

```{r}
lda_Y_test <- lda_test$class
```

The confusion matrix is:

```{r}
cm_LDA <- confusionMatrix(table(Y_test,lda_Y_test), positive="Positive")
cm_LDA$table %>% kable() %>% kable_styling(latex_options = "striped") %>% kable_styling(latex_options = "HOLD_position")
```

Computing the Accuracy and the TER might be interesting too.

```{r}
lda_TER <- mean(Y_test!=lda_Y_test)
lda_TER
lda_accuracy <- 1 - lda_TER
lda_accuracy
```
If we compute the BAC we can see that it is much better than the default logistic regression. 

```{r}
bac(cm_LDA$table)
```

The reason behind this is that, as said earlier, due to its bayesian nature, it takes into account that the problem is unbalanced.
If we compute its sensitivity and specificity:

```{r}
cm_LDA$byClass["Sensitivity"]
cm_LDA$byClass["Specificity"]
```
It can be seen that the specificity is much larger than the sensitivity. We can also compute the BAC by:

```{r}
bac(cm_LDA$table)
```


Recall can obtain the conditional probabilities of the classifications made with the test sample:
```{r}
prob_lda_Y_test <- lda_test$posterior
head(prob_lda_Y_test)
```

Thanks to this, we can treat it as a scoring classifier. In order to find the optimum threshold, we can plot a ROC curve:

```{r}
test_prob = predict(lda_train, newdata = X_test, type = "response")
test_prob <- test_prob$posterior[,2]

ROC_lda = plot.roc(Y_test, test_prob,
               main="Confidence interval of a threshold", percent=TRUE,
               ci=TRUE, of="thresholds", 
               thresholds="best", 
               print.thres="best",
               print.auc=TRUE) 
```

According to the ROC curve, the optimum threshold is:
```{r}
as.numeric(rownames(ROC_lda$ci$specificity))
```


If we compute the predictions with the new threshold:

```{r}
pred <- predict(lda_train, X_test, type = "response")$posterior[,2] > 
  as.numeric(rownames(ROC_lda$ci$specificity))
pred <- pred * 1
pred <- as.factor(c("Negative", "Positive")[(1*pred)+1])
cm_lda_opt <- confusionMatrix(table(Y_test, pred), positive="Positive")
cm_lda_opt$table %>% kable() %>% kable_styling(latex_options = "striped") %>% kable_styling(latex_options = "HOLD_position")
```

```{r, echo=FALSE}
predictors_tables[[3]] <- cm_lda_opt
```


We can also compute the BAC:

```{r}
BAC <- bac(cm_lda_opt$table)
BAC
```

It is similar to the BAC computed in the logistic regression after tuning the threshold.

Finally, we plot the probability of being positive of diabetes and the threshold:
```{r}
probs <- predict(lda_train, X_test, type = "response")$posterior[,2]
colors_errors <- c(c4,c3)[1*(Y_test==pred)+1]
plot(1:n_test,probs,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of Diabetes",
     main="Probabilities of Diabetes")
abline(h=as.numeric(rownames(ROC_lda$ci$specificity)))
```


### Quadraric discriminant analysis (QDA)

First, we train the model by:

```{r}
qda_train <- qda(Y_train~.,data=X_train)
```
The workflow for this method will be mostly the same as with the LDA.


We make the predictions as:

```{r}
qda_test <- predict(qda_train,newdata=X_test)
```

We can also save the predictions as:

```{r}
qda_Y_test <- qda_test$class
```

And the confusion table is:

```{r}
cm_QDA <- confusionMatrix(table(Y_test,qda_Y_test), positive="Positive")
cm_QDA$table %>% kable() %>% kable_styling(latex_options = "striped") %>% kable_styling(latex_options = "HOLD_position")
```
This is the same confusion table as for the default LDA! This means that accuracy, sensitivity, specificity... will be all the same.
```{r, echo=F}
predictors_tables[[4]] <- cm_QDA
```

Finally, we plot the probabilities of having diabetes and the default threshold.

```{r}
probs <- predict(qda_train, X_test, type = "response")$posterior[,2]
colors_errors <- c(c4,c3)[1*(Y_test==qda_Y_test)+1]
plot(1:n_test,probs,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of Diabetes",
     main="Probabilities of Diabetes")
abline(h=0.5)
```



### Naive Bayes (NB)

We start by training the naive-bayes model:

```{r}
nb_train <- gaussian_naive_bayes(X_train,Y_train)
```

And computing the predictions:

```{r}
nb_test <- predict(nb_train,newdata=as.matrix(X_test),type="prob")
```

Next, we generate the vector of classifications:
```{r}
nb_Y_test <- as.factor(ifelse(nb_test[,2]>0.5, "Positive", "Negative"))
```

By computing the confusion matrix, we can see that the method yields good results:

```{r}
cm_nb <- confusionMatrix(table(Y_test,nb_Y_test), positive = "Positive")
cm_nb$table
```
```{r, echo=FALSE}
predictors_tables[[5]] <- cm_nb
```

It may be interesting to compute TER, accuracy, sensitivity, specificity...

```{r}
nb_TER <- mean(Y_test!=nb_Y_test)
print(paste0("Test Error Rate: ",nb_TER))
nb_acc <- 1 - nb_TER
print(paste0("Accuracy: ",nb_acc))
cm_nb$byClass["Sensitivity"]
cm_nb$byClass["Specificity"]
```

The results are good, specially for the specificity. We can also compute the BAC:

```{r}
bac(cm_nb$table)
```
The BAC is also as good as the accuracy.
Ploting the probabilities of diabetes for each instance:
```{r}
test_prob <-  predict(nb_train, newdata = as.matrix(X_test), type = "prob")
prob_nb_Y_test <- test_prob[,2]
colors_errors <- c(c4,c3)[1*(Y_test==nb_Y_test)+1]
plot(1:n_test,prob_nb_Y_test,col=colors_errors,pch=20,type="p",
     xlab="Test sample",ylab="Probabilities of diabetes",
     main="Probabilities of diabetes")
abline(h=0.5)
```

We can see different threshold ploting the ROC curve:

```{r}
ROC_nb = plot.roc(Y_test, prob_nb_Y_test,
               main="Confidence interval of a threshold", percent=TRUE,
               ci=TRUE, of="thresholds",
               thresholds="best", 
               print.thres="best",
               print.auc=TRUE) 
ROC_nb
```

<!---
```{r}
nb_Y_test_tuned <- as.factor(ifelse(nb_test[,2]>as.numeric(rownames(ROC_lda$ci$specificity)), "Positive", "Negative"))
cm_nb_tuned <- confusionMatrix(table(Y_test,nb_Y_test_tuned), positive = "Positive")
cm_nb_tuned$table
```
-->



## Conclusions of the section

We have tried five different supervised classification methods:

- Logistic regression (LR)
- Linear discriminant analysis (LDA)
- Quadratic discriminant analysis (QDA)
- Naive Bayes (NB)
- K-Nearest Neighbors (KNN)

The results of the best classifier obtained for each of the methods can be seen in the following table:

```{r}
results_table <- data.frame(x=c(0,0,0,0))
for(i in 1:length(predictors_tables)){
  v <- c(predictors_tables[[i]]$overall["Accuracy"],
         bac(predictors_tables[[i]]$table),
         predictors_tables[[i]]$byClass["Sensitivity"],
         predictors_tables[[i]]$byClass["Specificity"]
         )
  results_table <- cbind(results_table, v)
}
results_table <- results_table[,-1]
rownames(results_table) <- c("Accuracy", "Balanced Accuracy", "Sensitivity", "Specificity")
colnames(results_table) <- c("KNN", "LR", "LDA", "QDA", "NB")

results_table %>% kable() %>% kable_styling(latex_options = "striped") %>% kable_styling(latex_options = "HOLD_position")
```

As we were faced with an unbalanced problem, it was not enough to look at metrics such as accuracy, and we had to consult others such as balanced accuracy, sensitivity or specificity. According to these metrics, the default LR did quite poorly, but after thresholding with the help of the ROC curve, we got really good results.

On the other hand, and as we have commented throughout this section, Bayesian methods have the advantage in this type of problem that they take into account the a priori probability, so that by default they already consider that the problem is unbalanced. Thanks to this, they achieve very good results by default, but, depending on the levels of sensitivity and specificity to be achieved, it may be a good idea to perform thresholding.

We believe that considering what levels of sensitivity and specificity we want to obtain is especially important in medical problems (such as the one we are facing), since, for certain problems, we may be especially interested in correctly classifying positive individuals (Sensitivity), in others negative individuals (Specificity), or in others in finding a balance between them (Balanced Accuracy).

# Conclusions





