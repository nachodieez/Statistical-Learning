---
title: "Team Project"
output: pdf_document
author: 
  - José Ignacio Díez Ruiz	100487766
  - Carlos Roldán Piñero	100484904
  - Pablo Vidal Fernández	100483812
date: "`r Sys.Date()`"
header-includes:
  - \renewcommand{\and}{\\}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
require(ggplot2)
require(tidyverse)
require(mice)
require(visdat)
require(MASS)
require(pracma)
require(rrcov)
require(corrplot)
require(GGally)
require(dbscan)
require(cluster)
require(mclust)
require(corpcor)
require(RSpectra)
require(factoextra)
require(knitr)
require(kableExtra)
```

<!-- TODO: Introduction -->

# Descriptive Analysis and Preprocessing

## Reading the data and setting the NAs

Before starting the analysis, we need to make some preprocessing on our data.
Let us start by loading it into memory and listing the names of the columns.

```{r}
data <- read.csv("diabetes.csv")
colnames(data)
```

Of these, our target variable is `Outcome`, which has two levels.
For convenience, we transform it into a factor variable which R can trat accordingly.

```{r}
data$Outcome <- factor(data$Outcome, c(0, 1), c("Negative", "Positive"))
```

<!-- Up to this point this would go in the introduction -->

Now we need to address a particularity of the chosen data:
not-a-number (NaN) instances are encoded as zeros in variables
where that value is imposible [^1].
These are:

- Glucose

- BloodPressure

- SkinThickness

- Insulin

- BMI

In order for us to later trat them correctly, we need to manually
change them to the existing `NA` type.
As we do so, we record the number of NaNs instances in each of those variables.
For convenience, we define a function.

[^1]: Remember we are dealing with medical data, not with artificial one.
Hence, there are constraints on the values a variable may take.

```{r}
set_nas <- function(data, fields) {
    percentage <- list()
    for (field in fields) {
        data[[field]][data[[field]] == 0]   <- NA
        percentage[[field]] <- 100 * sum(is.na(data[[field]])) / nrow(data)
    }
    return(list(data = data, percentage = percentage))
}

# Correctly label NaNs
na_fields   <- c("Glucose", "BloodPressure", "SkinThickness", "Insulin", "BMI")
data_na     <- set_nas(data, na_fields)
data        <- data_na$data
percentages <- data_na$percentage

# Visualize them
vis_miss(data)
```

Now the next logical step is to impute those NaN values.
We do have some concern about the imputation of the "Insulin" variable
which is almost half-filled of NaNs.
Nevertheless, we decide to impute it.
On the followed strategy, we use the "Predictive Mean Matching Imputation"
(PMM in short) as it behaves much more robustly than naive
mean or median imputations.

```{r}
pred_mat <- matrix(1, ncol(data), ncol(data), dimnames = list('row' = colnames(data), 'col' = colnames(data)))
pred_mat[,9] <- 0
for (i in 1:8){
  pred_mat[i,i]<-0
}
data_im <- mice(data, m = 1, method = "pmm", )
data    <- complete(data_im)
```

## Visualization of the data

Before we proceed any further, we are going to describe our data.
First, we look individually to each of the attributes,
we see both visually and with with a two-sample Wilcox-test whether there
is significant difference between the two groups (having or not diabetes),
and if some transformation may be desirable to ensure normality compatibility.
For that purpose, we define the following function.

```{r}
histogram_by_groups <- function(data, var, label = NULL) {
    stat_t <- wilcox.test(as.formula(paste(var, "~ Outcome")), data)
    data0  <- data[data$Outcome == "Negative", ]
    data1  <- data[data$Outcome == "Positive", ]
    if (is.null(label)) {
        label <- var
    }
    p <- ggplot(data0, aes(x = eval(parse(text = var)))) +
        geom_histogram(
            aes(y = after_stat(count / sum(count)), fill = "Negative"),
            bins = 10, colour = "white", alpha = 0.8, boundary = 0
            ) +
        geom_histogram(data = data1,
            aes(
                x = eval(parse(text = var)),
                y = after_stat(count / sum(count)), fill = "Positive"
                ),
            bins = 10, colour = "white",
            alpha = 0.5, boundary = 0, inherit.aes = FALSE) +
            theme_bw() +
            scale_fill_manual(
                name = "",
                breaks = c("Positive", "Negative"),
                values = c("Positive" = "deeppink4", "Negative" = "pink2")
            ) +
            xlab(label) + ylab("Relative frequency") + ggtitle(label) +
            geom_vline(xintercept = mean(data1[[var]]), colour = "deeppink4") +
            geom_vline(xintercept = mean(data0[[var]]), colour = "pink2")
    p + annotate(
            "text",
            x = 0.9 * max(data1[var]),
            y = 0.9 * max(ggplot_build(p)$data[[1]]["y"]),
            label = sprintf("p-value = %.4e", stat_t$p.value),
            size = 3
        )
}
```

Let us start with the number of pregnancies.
We can see that people who have diabetes have had more
pregnancies than those who do not have diabetes.
We see that it seems to be somewhat based on the p-value alone.
We also note it exhibits a heavily right-skewed behaviour.
As such, a logarithmic transformation would make sense to get
a distribution more compatible with the normal one.
Nonetheless, a problem here arises in dealing with the null values [^2].
For that purpose we shift the variable by one unit.
As a consistency measure, we will apply this shift to all
variables we log-transform.

[^2]: Remember the domain of the logarithmic function is $(0, \infty)$.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Pregnancies")
```

The change as we see does help in obtaining a more
centered distribution with which we may better apply
the posterior analysis.

```{r, fig.dim = c(8, 4)}
data$LogPregnancies <- log(data$Pregnancies + 1)
histogram_by_groups(data, "LogPregnancies")
```

The next variable to visualize is the glucose.
People with diabetes exhibit higher glucose values.
The p-value  is very small which
indicates a highly significant difference between the two groups.
We also observe that the distribution is already
well-centered and resembles a normal distribution.
Hence, we decide not to transform the data.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Glucose")
```

We move onwards to blood pressure.
In this case, although the distributions appear
as normal, there does not seem to exist a highly significant
difference between the groups in contrast to what the p-value states,
more so compared with the glucose variable.
Nonetheless, there seem to be an slight indication
of higher blood pressure for people with diabetes.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "BloodPressure")
```

Now skin thickness is interesting, because the distributions
are visually to those of the blood pressure but the presence
of outliers is appreciable.
We will later deal with those but for now let us
mantain this variable as it is.
Note that the population with diabetes appear
to exhibit a thicker skin.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "SkinThickness")
```

Insulin, as we may have expected from the name of the property
itself, appears to be a relevant.
The median of the distributions does indeed seem to differ,
with the one for the diabetes population being
slightly higher.
It is also right-skewed, so we decide to log-transform it.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Insulin")
```

It does appear that the transformation impproves the
the symmetrization of the data, although some left-skewness
appears.
We will later see if outlier detection get to
target those values or not.

```{r, fig.dim = c(8, 4)}
data$LogInsulin <- log(data$Insulin + 1)
histogram_by_groups(data, "LogInsulin")
```

Body Mass Index (BMI) again exhibits this tendency of
leaning towards a more right-skewed distribution.
It does also follow the tendency of being
slightly higher for people with diabetes.
As such we log-transform to try and get a more normalized variable.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "BMI")
```

As we may visually judge, it is the case
that the log transformation centers the data
and provides a more normal-like distribution.
There is some presence of seemingly outlying points
to the right.

```{r, fig.dim = c(8, 4)}
data$LogBMI <- log(data$BMI + 1)
histogram_by_groups(data, "LogBMI")
```

The Diabetes Pedigree Function (DPF) is the
again a flagrant right-skewed.
It again has higher values for the positive set.
This is to be expected from the definition of this
very function as a risk indication for diabetes.
Let us try to log-transform it.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "DiabetesPedigreeFunction", "DPF")
```

The improvement is highly noticeable.
We retain thus this transformed variable.

```{r, fig.dim = c(8, 4)}
data$LogDPF <- log(data$DiabetesPedigreeFunction + 1)
histogram_by_groups(data, "LogDPF")
```

We may note that young people, as with other illnesses
have less tendency to suffer diabetes than the elders.
The age is expected to exhibit a right-skewed
distribution, as is indeed the case.
In an attempt to improve the symmetry,
we once again use logarithms to transform the variable.

```{r, fig.dim = c(8, 4)}
histogram_by_groups(data, "Age")
```

The transformation does help although
not by much.
This is a common problem of the age variable.
We keep the transformation anyway as it does seem
to help with the symmetry for the positive group.

```{r, fig.dim = c(8, 4)}
data$LogAge <- log(data$Age + 1)
histogram_by_groups(data, "LogAge")
```

```{r}
# Some convenience
df  <- subset(data, select = -c(Pregnancies, Insulin, BMI, DiabetesPedigreeFunction, Age))
df0     <-  df[df$Outcome == "Negative", ]
df1     <-  df[df$Outcome == "Positive", ]
xnames  <- names(df)[! names(df) %in% c("Outcome")]
```

Now, let's take a look at some multivariate plots. We'll begin by inspecting the Parallel Coordinate Plot:

```{r, fig.dim = c(8, 4)}
par(las = 2)
parcoord(df[xnames], col = c("pink2", "deeppink4"))
legend("topright", legend = c("Negative", "Positive"),
       col = c("pink2", "deeppink4"), lty = 1, lwd = 2)
```

It seems that, overall, the positive lines are 
over negative ones. 
This is most notable on the Glucose and log
transformed BMI.
They are two of the most significant
features according to the p-value.

The Andrew's plot is the following: 

```{r, fig.dim = c(8, 4)}
andrewsplot(as.matrix(df[xnames]), df$Outcome, style = "cart")
legend("topright", legend = c("Negative", "Positive"),
       col = c("black", "red"), lty = 1, lwd = 2)
```

Again, we see that the two groups are different.
The group of people who have diabetes tend to have more volatile curves,
reaching higher and lower values along the curve.

## Multivariate characteristics and outlier identification

We are goig to use a multivariate approach
to identifying the outliers in our data.
In the process, we will need to compute the
mean and covariance.

We start then by finding the mean vector and the
covariance matrix.
In order to reduce the sensitivity to outliers in this
computation, we use a robust estimation using 
the "Fast MCD" (Minimum Covariance Determinant) estimator.
We set main parameter, `alpha`, which determines the percentage
of the data to use, at $0.85$.

<!-- TODO: refactor the prints -->

```{r}
our_corrplot <- function(cov_mat) {
    colnames(cov_mat) <- c("Log\nPregnancies",
                           "Glucose",
                           "Blood\nPressure",
                           "Skin\nThickness",
                           "LogInsulin",
                           "LogBMI",
                           "LogDPF",
                           "LogAge")
    corrplot.mixed(cov2cor(cov_mat), lower = "number", upper = "color",
        diag = "n", tl.col = "black", tl.cex = 0.65,
        lower.col = "black")
}

mcd_est <- CovMcd(df[xnames], alpha = 0.85, nsamp = "deterministic")
```

```{r, echo = FALSE, fig.size = c(8,8)}
print("The mean vector is:")
print(mcd_est$center)
print("The covariance matrix is:")
print(round(mcd_est$cov, 4))
```

```{r, fig.size = c(8,8)}
our_corrplot(mcd_est$cov)
```

It is interesting to seggregate by the class
of the Outcome variable.
This way, we can both, get the mean vector and covariance
matrix for each of the classes, and the outliers for both sets.

```{r, fig.size = c(8,8)}
mcd_neg <- CovMcd(df0[xnames], alpha = 0.85, nsamp = "deterministic")
mcd_pos <- CovMcd(df1[xnames], alpha = 0.85, nsamp = "deterministic")
```

```{r, echo = FALSE, fig.size = c(8,8)}
print("### Negative class ###")
print("The mean vector is:")
print(mcd_neg$center)
print("The covariance matrix is:")
print(round(mcd_neg$cov, 4))
```

```{r, fig.size = c(8,8)}
our_corrplot(mcd_neg$cov)
```

```{r, echo = FALSE, fig.size = c(8,8)}
print("### Positive class ###")
print("The mean vector is:")
print(mcd_pos$center)
print("The covariance matrix is:")
print(round(mcd_pos$cov, 4))
```

```{r, fig.size = c(8,8)}
our_corrplot(mcd_pos$cov)
```

Let us focus first on the mean vectors.
The here observed particularities are nothing new,
as they were already present on above histograms:

- Higher number of pregnancies seems to increase
the chances on developing diabetes.

- The glucose and insulin levels of the positive
population is higher in contrast to the negative one.

Looking now at the representations for the covariance matrices,
the major changes are that the correlation between skin thickness and 
diabetes pedigree function is lower in the group who do not have diabetes
than in the one having the disease.
Moreover, the correlation between BMI [^3] and age is positive 
for the sane group while negative on the positive one.

[^3]: Remember this is the logarithm.

We now search for outliers.
The idea is to use Mahalanobis distance.
As we suppose that our data $X \sim \mathcal{N}(\mu, \Sigma)$,
we have $D_M(x,\mu)^2 \sim \chi_p^2$, with $D_M$ the
Mahalanobis distance.
Hence we may set our outlier criteria as
\begin{equation}
    D_M(x, \mu)^2 > \chi_{p, 0.95^{1/n}}^2 \;,
\end{equation}
the $0.95^\text{th}$ quantile of the $\chi_p^2$ distribution.
We then drop these outliers.

```{r}
p   <- length(xnames)
n0  <- nrow(df0)
n1  <- nrow(df1)
df0_clean <- df0[mcd_neg$mah < qchisq(0.95^(1 / n0), p), ]
df1_clean <- df1[mcd_pos$mah < qchisq(0.95^(1 / n1), p), ]
df_clean  <- rbind(df0_clean, df1_clean)
```

```{r, echo = FALSE}
print(paste("The negative set contained", n0 - nrow(df0_clean), "outliers."))
print(paste("The positive set contained", n1 - nrow(df1_clean), "outliers."))
```

As a final summary of this section, we
perform a plot in which the histograms of different populations
are observed, as well as scatterplots of pairs of variables
and correlations.

```{r, fig.dim=c(16,16)}
ggpairs(df_clean, aes(color = Outcome), legend = 1,
        columns = xnames,
        diag = list(continuous = "barDiag")
        ) +
    theme(legend.position = "bottom") +
    scale_fill_manual(values = c("pink", "deeppink4")) +
    scale_color_manual(values = c("pink", "deeppink4")) + labs(fill = "Outcome")
```

<!-- TODO: PCA -->
<!-- definitive data: df_clean -->
<!-- definitive attributes: df_clean[, xnames] -->
<!-- definitive objective variable: df_clean$Outcome -->

```{r, echo = F}
c1 <- "deeppink4"
c2 <- "gold2"
c3 <- "darkorchid4"
c4 <- "seagreen2"
c5 <- "orange2"
c6 <- "firebrick2"

X <- df_clean[,-4]
y <- df_clean[,4]
y2 <- dplyr::recode(y, Negative = 0, Positive = 1)
X2 <- scale(X)
```

# Unsupervised learning

```{r, echo = F}
cluster_means <- function(x, data = X, first = 1){
  if(first == 0){
    x <- x + 1
  }
  ls <- list()
  groups_ <- sort(unique(x))
  for (i in groups_){
    ls[[i]] <- data[x == i,]
  }
  z <- sapply(ls, function(x) sapply(x, mean, na.rm = T))
  z <- as.data.frame(z)
  colnames(z) <- paste("Cluster", 1:length(groups_), sep = " ")
  z2 <- z
  return(z2)
}

cluster_classification <- function(x, y = y2, first = 1){
  x <- x - first
  prev <- table(x, y)
  if(prev[1,1] + prev[2,2] > prev[1,2] + prev[2,1]){
    f <- prev
  } else{
    f <- table(1-x, y)
  }
  dimnames(f) <- list("Pred" = c("Negative", "Positive"), "Actual" = c("Negative", "Positive"))
  f2 <- f
  return(f)
}
```

## Partitional clustering

### Number of clusters

We'll firstly decide how many clusters we want to create. We'll use the silhoutte method and the gap statistic. The silhoutte method:

```{r}
fviz_nbclust(X2, kmeans, method = "silhouette", k.max = 10)
```
As we can see, this indicates that the optimum number of clusters is 2.

The gap statistic:

```{r, results = "hide"}
gap_stat <- clusGap(X2, FUN = kmeans, K.max = 10, B = 100)
```
```{r}
fviz_gap_stat(gap_stat, linecolor = c1, maxSE = 
                list(method = "firstSEmax", SE.factor = 1))
```
This also suggests that the optimal number of clusters is 2.

### K-Means

The results of k-means clustering are the following: 

```{r}
kmeans_X <- kmeans(X2, centers = 2, iter.max = 1000, nstart = 100)

kmeans_col <- c(c1, c2)[kmeans_X$cluster]
plot(X2[,1:2], col = kmeans_col, main = "First two PCs", 
     xlab = "First PC", ylab = "Second PC", pch  = 16)
```

Let's see the average in each variable for the clusters:

```{r}
kmeans_means <- cluster_means(kmeans_X$cluster)
kable(kmeans_means)
```

We can also see the confusion matrix with our target variable, although we must remember that this is not the objective of unsupervised learning.

```{r}
kable(cluster_classification(kmeans_X$cluster, first = 1))
```

As we can see, the clusters are not exactly coincident with the target variable.

## K-Medoids

As all of our attributes are numerical and we don't have any reason not to, we will use Euclidean distance.

```{r}
pam_X <- pam(X2, k = 2, metric = "euclidean", stand = FALSE)
pam_col <- c(c1, c2)[pam_X$cluster]

plot(X2[,1:2], col = pam_col, main = "First two PCs", 
     xlab = "First PC", ylab = "Second PC", pch = 19)
```

We obtain a very similar result than with K-Means.

```{r}
pam_means <- cluster_means(pam_X$cluster, first = 1)
kable(pam_means)
```
Again, the confusion matrix is the following:

```{r}
cluster_classification(pam_X$cluster)
```
Very similar results than with K-Means.

### CLARA

```{r}
clara_X <- clara(X2, k = 2, metric = "euclidean", stand=FALSE)
clara_col <- c(c1, c2)[clara_X$cluster]

plot(X2[,1:2], col = clara_col, main = "First two PCs", 
     xlab = "First PC", ylab = "Second PC", pch = 19)
```
Again, we can observe that the results are very similar than with both K-Means and K-Medoids.

```{r}
clara_means <- cluster_means(clara_X$cluster, first = 1)
kable(clara_means)
```

```{r}
cluster_classification(clara_X$cluster)
```
## Hierarchical clustering

### Single linkage

### Complete linkage

### Average linkage

### Ward linakge

## DBSCAN

```{r}
minPts <- 5
kNNdistplot(X2, k = minPts - 1)
abline(h = 1.8, col = c4)
```

```{r}
db_fit <- dbscan(X2, eps = 1.8, minPts = 5)
colors_db <- c(c1, c2, c3)[db_fit$cluster + 1]
plot(X2[,1:2], pch = 19, col = colors_db, xlab = "First PC",
     ylab = "Second PC")
```

```{r}
table(db_fit$cluster)
```

The clusters have very different sizes, and this is not a good indicator of a good split. The averages are:

```{r, echo = F}
db_means <- cluster_means(db_fit$cluster, first = 0)
kable(db_means)
```
We can also see the confusion matrix with our target variable, although we must remember that this is not the objective of unsupervised learning.

```{r}
kable(cluster_classification(db_fit$cluster, first = 0))
```

## Model-based clustering

```{r}
BIC_X <- mclustBIC(X2, G = 1:5)
plot(BIC_X)
summary(BIC_X)
```

The suggested number of clusters is 4. However, we can see that there is not much difference between choosing 2, 3, 4 and 5. The best models are EEE (ellipsoidal, equal volume, shape and orientation) with 4 clusters, and VEE (ellipsoidal, equal shape and orientation) with 4 and 5 clusters. 

```{r}
Mclust_X <- Mclust(X2, x = BIC_X, verbose = F)
summary(Mclust_X)
```

```{r}
mb_means <- cluster_means(Mclust_X$classification, first = 1)
kable(mb_means)
```









